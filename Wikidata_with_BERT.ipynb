{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version :  1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"  \n",
    "\n",
    "# import the tokenizer which is used on this pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch version : \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  28996\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"vocab size : \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'yokozeki was born in osaka prefecture on september 11, 1979'\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# print(text)\n",
    "# print(tokens[:10], '...')\n",
    "# print(ids[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_place_of_birth / data_place_of_death / data_occupation\n",
    "\n",
    "data_0 = pd.read_csv('./data/data_occupation_0.csv')\n",
    "data_1 = pd.read_csv('./data/data_occupation_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0_train = data_0[0:3000]\n",
    "data_0_val = data_0[3000:6000]\n",
    "data_0_test = data_0[6000:9000]\n",
    "\n",
    "data_1_train = data_1[0:3000]\n",
    "data_1_val = data_1[3000:6000]\n",
    "data_1_test = data_1[6000:9000]\n",
    "\n",
    "df_train = shuffle(data_0_train.append(data_1_train, ignore_index=True))\n",
    "df_val = shuffle(data_0_val.append(data_1_val, ignore_index=True))\n",
    "df_test = shuffle(data_0_test.append(data_1_test, ignore_index=True))\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (6000, 3)\n",
      "Validation Set: (6000, 3)\n",
      "Test Set: (6000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\"% df_train.columns, df_train.shape)\n",
    "print(\"Validation Set:\"% df_val.columns, df_val.shape)\n",
    "print(\"Test Set:\"% df_test.columns, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category best musical or comedy actor golden g...</td>\n",
       "      <td>occupation actor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yearaward categorytitleresultsref.1947 academy...</td>\n",
       "      <td>occupation writer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sean patrick hayes born june 26, 1970 is an am...</td>\n",
       "      <td>occupation film producer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peruggi did not accompany him to washington wh...</td>\n",
       "      <td>occupation writer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retrieved 2015-03-27 on 21 december 2014, chan...</td>\n",
       "      <td>occupation actor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  category best musical or comedy actor golden g...   \n",
       "1  yearaward categorytitleresultsref.1947 academy...   \n",
       "2  sean patrick hayes born june 26, 1970 is an am...   \n",
       "3  peruggi did not accompany him to washington wh...   \n",
       "4  retrieved 2015-03-27 on 21 december 2014, chan...   \n",
       "\n",
       "                   wikidata  label  \n",
       "0          occupation actor      0  \n",
       "1         occupation writer      1  \n",
       "2  occupation film producer      0  \n",
       "3         occupation writer      1  \n",
       "4          occupation actor      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data\n",
      "Number of sentences :  6000\n",
      "Longest sentence's length : 4825\n",
      "Average length of the sentences : 179.35866666666666\n"
     ]
    }
   ],
   "source": [
    "print('For train data')\n",
    "print(\"Number of sentences : \", len(df_train))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_train.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_train.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For validation data\n",
      "Number of sentences :  6000\n",
      "Longest sentence's length : 4467\n",
      "Average length of the sentences : 176.75933333333333\n"
     ]
    }
   ],
   "source": [
    "print('For validation data')\n",
    "print(\"Number of sentences : \", len(df_val))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_val.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_val.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data\n",
      "Number of sentences :  6000\n",
      "Longest sentence's length : 3520\n",
      "Average length of the sentences : 176.91333333333333\n"
     ]
    }
   ],
   "source": [
    "print('For test data')\n",
    "print(\"Number of sentences : \", len(df_test))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_test.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_test.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the tweet max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_text_length(text, length):\n",
    "    text = text[:length]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baldwin was nominated for an academy award, a ...</td>\n",
       "      <td>occupation film producer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it was selected as one of the best science fic...</td>\n",
       "      <td>occupation writer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pope john iv ; died 12 october 642 was the bis...</td>\n",
       "      <td>occupation politician</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soon, franquin was considered an undisputed ma...</td>\n",
       "      <td>occupation animator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>while it is believed four died early, the last...</td>\n",
       "      <td>occupation poet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>she received her only nomination for a filmfar...</td>\n",
       "      <td>occupation actor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>william sami étienne grigahcine ; born 13 june...</td>\n",
       "      <td>occupation disc jockey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>june 18, 2004 was an american helicopter engin...</td>\n",
       "      <td>occupation engineer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>justice harlan was very close to the law clerk...</td>\n",
       "      <td>occupation judge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>bernard boursicot born on 12 august 1944 is a ...</td>\n",
       "      <td>occupation diplomat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     baldwin was nominated for an academy award, a ...   \n",
       "1     it was selected as one of the best science fic...   \n",
       "2     pope john iv ; died 12 october 642 was the bis...   \n",
       "3     soon, franquin was considered an undisputed ma...   \n",
       "4     while it is believed four died early, the last...   \n",
       "...                                                 ...   \n",
       "5995  she received her only nomination for a filmfar...   \n",
       "5996  william sami étienne grigahcine ; born 13 june...   \n",
       "5997  june 18, 2004 was an american helicopter engin...   \n",
       "5998  justice harlan was very close to the law clerk...   \n",
       "5999  bernard boursicot born on 12 august 1944 is a ...   \n",
       "\n",
       "                      wikidata  label  \n",
       "0     occupation film producer      1  \n",
       "1            occupation writer      0  \n",
       "2        occupation politician      1  \n",
       "3          occupation animator      0  \n",
       "4              occupation poet      0  \n",
       "...                        ...    ...  \n",
       "5995          occupation actor      0  \n",
       "5996    occupation disc jockey      0  \n",
       "5997       occupation engineer      0  \n",
       "5998          occupation judge      0  \n",
       "5999       occupation diplomat      0  \n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentence'] = df_test['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yi is the author of ten books, and he served a...</td>\n",
       "      <td>occupation author</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he studied engineering at the technical univer...</td>\n",
       "      <td>occupation architect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>winnie hsin ; born 8 february 1962 is a taiwan...</td>\n",
       "      <td>occupation writer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>, also spelled nukada, was a japanese poet of ...</td>\n",
       "      <td>occupation poet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>during the final eight days of the expedition,...</td>\n",
       "      <td>occupation theologian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>2002 underground zero — director segment \"isai...</td>\n",
       "      <td>occupation screenwriter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>2007 korean model awards model star award, fas...</td>\n",
       "      <td>occupation television presenter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>2010 jupiter award – best male actor tv for hi...</td>\n",
       "      <td>occupation screenwriter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>originally ellsworth did not have a first name...</td>\n",
       "      <td>occupation film producer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>after completing his primary education at the ...</td>\n",
       "      <td>occupation playwright</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     yi is the author of ten books, and he served a...   \n",
       "1     he studied engineering at the technical univer...   \n",
       "2     winnie hsin ; born 8 february 1962 is a taiwan...   \n",
       "3     , also spelled nukada, was a japanese poet of ...   \n",
       "4     during the final eight days of the expedition,...   \n",
       "...                                                 ...   \n",
       "5995  2002 underground zero — director segment \"isai...   \n",
       "5996  2007 korean model awards model star award, fas...   \n",
       "5997  2010 jupiter award – best male actor tv for hi...   \n",
       "5998  originally ellsworth did not have a first name...   \n",
       "5999  after completing his primary education at the ...   \n",
       "\n",
       "                             wikidata  label  \n",
       "0                   occupation author      0  \n",
       "1                occupation architect      1  \n",
       "2                   occupation writer      1  \n",
       "3                     occupation poet      0  \n",
       "4               occupation theologian      1  \n",
       "...                               ...    ...  \n",
       "5995          occupation screenwriter      1  \n",
       "5996  occupation television presenter      1  \n",
       "5997          occupation screenwriter      1  \n",
       "5998         occupation film producer      0  \n",
       "5999            occupation playwright      1  \n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['sentence'] = df_val['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences that exceed 300 characters\n",
      "546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category best musical or comedy actor golden g...</td>\n",
       "      <td>occupation actor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sean patrick hayes born june 26, 1970 is an am...</td>\n",
       "      <td>occupation film producer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peruggi did not accompany him to washington wh...</td>\n",
       "      <td>occupation writer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>retrieved 2015-03-27 on 21 december 2014, chan...</td>\n",
       "      <td>occupation actor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>justin haber born 9 june 1981 is a maltese pro...</td>\n",
       "      <td>occupation seiyū</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>frans michel penning 12 september 1894 – 6 dec...</td>\n",
       "      <td>occupation politician</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>vladimir volfovich zhirinovsky ; né eidelstein...</td>\n",
       "      <td>occupation military personnel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>alexander vlahos born 30 july 1988 is a welsh ...</td>\n",
       "      <td>occupation inventor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>anies rasyid baswedan born 7 may 1969 is an in...</td>\n",
       "      <td>occupation peasant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>oxford university press, accessed november 14,...</td>\n",
       "      <td>occupation violinist</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5454 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     category best musical or comedy actor golden g...   \n",
       "2     sean patrick hayes born june 26, 1970 is an am...   \n",
       "3     peruggi did not accompany him to washington wh...   \n",
       "4     retrieved 2015-03-27 on 21 december 2014, chan...   \n",
       "5     justin haber born 9 june 1981 is a maltese pro...   \n",
       "...                                                 ...   \n",
       "5994  frans michel penning 12 september 1894 – 6 dec...   \n",
       "5996  vladimir volfovich zhirinovsky ; né eidelstein...   \n",
       "5997  alexander vlahos born 30 july 1988 is a welsh ...   \n",
       "5998  anies rasyid baswedan born 7 may 1969 is an in...   \n",
       "5999  oxford university press, accessed november 14,...   \n",
       "\n",
       "                           wikidata  label  \n",
       "0                  occupation actor      0  \n",
       "2          occupation film producer      0  \n",
       "3                 occupation writer      1  \n",
       "4                  occupation actor      1  \n",
       "5                  occupation seiyū      1  \n",
       "...                             ...    ...  \n",
       "5994          occupation politician      1  \n",
       "5996  occupation military personnel      1  \n",
       "5997            occupation inventor      1  \n",
       "5998             occupation peasant      1  \n",
       "5999           occupation violinist      0  \n",
       "\n",
       "[5454 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 6000\n",
    "MAX_LENGTH = 300\n",
    "print('Number of sentences that exceed 300 characters')\n",
    "print(L - len(df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]))\n",
    "\n",
    "# print(\"Number of tweets : \", len(df_train))\n",
    "# df_train['sentence'] = df_train['sentence'].map(lambda x: max_text_length(x, 300))\n",
    "df_train = df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_val.to_csv(\"validation.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category best musical or comedy actor golden globe television winners\n",
      "occupation actor\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "T = pd.read_csv('train.csv')\n",
    "T1, T2, T3= T.iloc[0, :].values\n",
    "print(T1)\n",
    "print(T2)\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    # read the tsv we make and initialize some parameters\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"validation\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \".csv\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # use BERT tokenizer\n",
    "    \n",
    "    # define a function that reutrn a training or testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\" or self.mode == \"validation\":\n",
    "            sentence, wikidata = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            sentence, wikidata, label_id = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(sentence)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        tokens_b = self.tokenizer.tokenize(wikidata)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # convert hole token sequence into index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # segments_tensor\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "trainset = WikiDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original document]\n",
      "sentence  ：category best musical or comedy actor golden globe television winners\n",
      "wikidata  ：occupation actor\n",
      "label ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[ tensors return by Dataset ]\n",
      "tokens_tensor  ：tensor([  101,  4370,  1436,  2696,  1137,  3789,  2811,  5404, 12868,  1778,\n",
      "         5222,   102,  5846,  2811,   102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[tokens_tensors word pieces]\n",
      "[CLS]categorybestmusicalorcomedyactorgoldenglobetelevisionwinners[SEP]occupationactor[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select first sample\n",
    "sample_idx = 0\n",
    "\n",
    "# compare with the original document\n",
    "sentence_0, wikidata_0, label_0 = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# use the Dataset we built to extract the transformed id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# convert the tokens_tensor to original document\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[original document]\n",
    "sentence  ：{sentence_0}\n",
    "wikidata  ：{wikidata_0}\n",
    "label ：{label_0}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[ tensors return by Dataset ]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[tokens_tensors word pieces]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a DataLoader that can return a mini-batch each time\n",
    "This DataLoader works with the 'WikiDataset' we define previously\n",
    "we need 4 tensors when training a BERT model：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The input samples of this function is a list,\n",
    "# every element in it is a sample return by the 'WikiDataset'\n",
    "\n",
    "# Every sample contains 3 tensors : \n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "\n",
    "# It will procecss zero padding on the first two tensors,\n",
    "# then create a masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # with labels or not\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pading\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # attention masks, \n",
    "    # set the locations that are not zero padding tokens_tensors to 1 in order to let bert only focus on those tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataLoader\n",
    "# use `collate_fn` to combine list of samples into a mini-batch\n",
    "BATCH_SIZE = 24\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([24, 72]) \n",
      "tensor([[  101,  4370,  1436,  ...,     0,     0,     0],\n",
      "        [  101,  2343,  1179,  ...,     0,     0,     0],\n",
      "        [  101,  1679,  9610,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 24181,  7702,  ...,     0,     0,     0],\n",
      "        [  101,  1113,  1185,  ...,     0,     0,     0],\n",
      "        [  101,  5871,  3740,  ...,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([24, 72])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([24, 72])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([24])\n",
      "tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# n_class = 2\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # first 3 tensors are tokens, segments and masks \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # calculate accuracy when training\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # record current batch\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# run the model on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "# _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "# print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of parameters of whole model：108311810\n",
      "number of parameters of the linear classifier：1538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "number of parameters of whole model：{sum(p.numel() for p in model_params)}\n",
    "number of parameters of the linear classifier：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 64.420, acc: 0.964\n",
      "[epoch 2] loss: 26.406, acc: 0.973\n",
      "[epoch 3] loss: 18.959, acc: 0.974\n",
      "Wall time: 5min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "\n",
    "# using Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 3    # number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # record batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # calculate accuracy\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'B:/Documents/2020五上/Wikidata/Wikipedia_Wikidata_Alignment/model_place_of_birth'\n",
    "# torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiset = WikiDataset(\"validation\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiloader = DataLoader(valiset, batch_size=32, collate_fn=create_mini_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    validations = get_predictions(model, valiloader)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations_numpy = validations.cpu().clone().numpy()\n",
    "validations_numpy = validations_numpy.reshape((6000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1,6001)\n",
    "ids = ids.reshape((6000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    0],\n",
       "       [   2,    1],\n",
       "       [   3,    1],\n",
       "       ...,\n",
       "       [5998,    1],\n",
       "       [5999,    0],\n",
       "       [6000,    1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations_array = np.concatenate((ids, validations_numpy), axis=1)\n",
    "validations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validations = pd.DataFrame(data = validations_array, columns=[\"id\", \"label\"])\n",
    "df_validations['label'] = df_validations['label'].map({0:0, 1:1})\n",
    "# df_validations.to_csv('answer.txt', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 :  0.9461907963061191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 : \",f1_score(df_validations['label'], df_val['label'],  average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2902   98]\n",
      " [ 225 2775]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(df_val['label'], df_validations['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
