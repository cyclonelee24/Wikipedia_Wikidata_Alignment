{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input,GRU,LSTM,Dense,Conv2D,AveragePooling1D,TimeDistributed,Flatten,MaxPooling2D,MaxPooling1D,Convolution1D,Reshape,Dropout,Embedding,Permute,Lambda,Multiply\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize glove to be the initial word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'glove.6B.50d.txt'\n",
    "output_file = r'gensim_glove.6B.50d.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0 = pd.read_csv(\"data_place_of_birth_0.csv\")\n",
    "f_1 = pd.read_csv('data_place_of_birth_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0 = f_0[:500]\n",
    "f_1 = f_1[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f_0.append(f_1)\n",
    "f = f.reset_index(drop=True)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = f[\"sentence\"].tolist()\n",
    "wikid = f[\"wikidata\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the stopwords for sentences and wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "EngStopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "drop_stop = []\n",
    "for p in range(0,len(plain)):\n",
    "    j = []\n",
    "    lower = plain[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    drop_stop.append(d)\n",
    "    \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata = []\n",
    "for p in range(0,len(wikid)):\n",
    "    j = []\n",
    "    lower = wikid[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    \n",
    "    wikidata.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "plain=[]\n",
    "\n",
    "for i in range(0,len(drop_stop)):\n",
    "    tokens = word_tokenize(drop_stop[i])  \n",
    "    tagged_sent = nltk.pos_tag(tokens)    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) \n",
    "    delimiter = ' '\n",
    "    ff = delimiter.join(lemmas_sent)\n",
    "    plain.append(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let wikidata and sentence to have their GloVe word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = len(wikidata)\n",
    "sl = len(plain)\n",
    "plain_e = []\n",
    "wikidata_e = []\n",
    "\n",
    "for i in range(len(wikidata)):\n",
    "    a = wikidata[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > wl:\n",
    "        w = w[0:wl]\n",
    "    else:\n",
    "        for k in range(wl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    wikidata_e.append(w)\n",
    "    \n",
    "for i in range(len(plain)):\n",
    "    a = plain[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > sl:\n",
    "        w = w[0:sl]\n",
    "    else:\n",
    "        for k in range(sl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    plain_e.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Layer\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nsmnattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(nsmnattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(nsmnattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        U = Permute((2,1))(x[0])\n",
    "        V = Permute((2,1))(x[1])\n",
    "        print(\"U.shape\",U.shape)\n",
    "        print(\"V.shape\",V.shape)\n",
    "        \n",
    "        E = k.batch_dot(Permute((2,1))(U),V)\n",
    "        \n",
    "        print(\"E.shape\",E.shape)\n",
    "        \n",
    "        U1 = k.batch_dot(V,Permute((2,1))((E)))     \n",
    "        \n",
    "        V1 = k.batch_dot(U,E)\n",
    "\n",
    "        U = Permute((2,1))(U)\n",
    "        U1 = Permute((2,1))(U1)\n",
    "        V = Permute((2,1))(V)\n",
    "        V1 = Permute((2,1))(V1)\n",
    "        S = Permute((2,1))((tf.keras.layers.concatenate([U,U1,(U-U1),Multiply()([U,U1])])))\n",
    "        T = Permute((2,1))((tf.keras.layers.concatenate([V,V1,(V-V1),Multiply()([V,V1])])))\n",
    "        print(\"S.shape\",S.shape)\n",
    "        print(\"T.shape\",T.shape)\n",
    "                        \n",
    "        P = LSTM(10,return_sequences=True)(S)\n",
    "        Q = LSTM(10,return_sequences=True)(T)\n",
    "        print(\"P.shape\",P.shape)\n",
    "        print(\"Q.shape\",Q.shape)\n",
    "                  \n",
    "        p = MaxPooling1D((40))(P)\n",
    "        q = MaxPooling1D((40))(Q)\n",
    "        \n",
    "        print(\"p.shape\",p.shape)\n",
    "        print(\"q.shape\",q.shape)\n",
    "        \n",
    "        m = tf.keras.layers.concatenate([p,q,(p-q),Multiply()([p,q])])\n",
    "        print(\"m.shape\",m.shape)\n",
    "        print('')\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(coattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(coattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        C = x[0]\n",
    "       \n",
    "        print(\"C.shape\",C.shape)\n",
    "        RNN=Permute((2,1))(x[1])\n",
    "        \n",
    "        f = k.dot(C,self.kernelW)\n",
    "        print(\"f.shape\",f.shape)\n",
    "        F = k.tanh(k.batch_dot(f,RNN))\n",
    "        print(\"F.shape\",F.shape)\n",
    "        \n",
    "        s = k.dot(RNN,self.kernelWs)\n",
    "        print(\"s.shape\",s.shape)\n",
    "        c = k.dot(Permute((2,1))(C),self.kernelWc)\n",
    "        print(\"c.shape\",c.shape)\n",
    "       \n",
    "        Hs = k.tanh(s+k.batch_dot(c,F))\n",
    "        print(\"Hs.shape\",Hs.shape)\n",
    "        Hc = k.tanh(c+k.batch_dot(s,Permute((2,1))(F)))\n",
    "        print(\"Hc.shape\",Hc.shape)\n",
    "        \n",
    "        \n",
    "        As = k.softmax(k.dot(Permute((2,1))(Hs),self.kernelas))\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = k.softmax(k.dot(Permute((2,1))(Hc),self.kernelac))\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        As = Permute((2,1))(As)\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = Permute((2,1))(Ac)\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        sfinal = k.batch_dot(As,Permute((2,1))(RNN))\n",
    "        print(\"sfinal.shape\",sfinal.shape)\n",
    "        \n",
    "        cfinal = k.batch_dot(Ac,C)\n",
    "        print(\"cfinal.shape\",cfinal.shape)\n",
    "        print('')\n",
    "        \n",
    "        return tf.keras.layers.concatenate([sfinal,cfinal])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(plain_e, y , test_size=0.2, random_state=1000)\n",
    "X_train_1, X_test_1, y_train, y_test = train_test_split(wikidata_e, y , test_size=0.2, random_state=1000)\n",
    "\n",
    "y_train = to_categorical(y_train,2)\n",
    "y_train = y_train.astype('int')\n",
    "y_train = y_train.reshape(-1, 1, 2)\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.astype('int')\n",
    "y_test = y_test.reshape(-1, 1, 2)\n",
    "\n",
    "wl = len(wikidata)\n",
    "sl = len(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winput = Input(shape=(sl,50))\n",
    "wembed = LSTM(10,return_sequences=True)(winput)\n",
    "\n",
    "winput_1 = Input(shape=(wl,50))\n",
    "wembed_1 = LSTM(10,return_sequences=True)(winput_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "co = nsmnattention(40)([wembed, wembed_1])\n",
    "co = Dense(2)(co)\n",
    "coc = coattention(20)([wembed, wembed_1])\n",
    "coc = Dense(2)(coc)\n",
    "c = tf.keras.layers.concatenate([co, coc])\n",
    "output = Dense(2)(c)\n",
    "output = Dense(2, activation=\"softmax\")(output)\n",
    "\n",
    "model = Model([winput, winput_1], [output])\n",
    "model.summary()\n",
    "\n",
    "RMSprop = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=RMSprop, loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Accuracy()], experimental_run_tf_function=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "history = model.fit([np.array(X_train), np.array(X_train_1)], [np.array(y_train)],\n",
    "                  epochs=20, validation_split=0.2, callbacks=[early_stopping], batch_size=64)\n",
    "\n",
    "scores = model.evaluate([np.array(X_test), np.array(X_test_1)], np.array(y_test), verbose=0)\n",
    "pre = model.predict([np.array(X_test), np.array(X_test_1)])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_pre = []\n",
    "\n",
    "for i in range(len(pre)):\n",
    "    k = pre[i]\n",
    "    w = np.where(k == np.max(k))[0][0].tolist()\n",
    "    y_pre.append(w)\n",
    "\n",
    "    \n",
    "y = f[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(plain_e, y, test_size=0.2, random_state=1000)\n",
    "X_train_1, X_test_1, y_train, y_test = train_test_split(wikidata_e, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pre))\n",
    "\n",
    "print('Weighted precision', precision_score(y_test, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted recall', recall_score(y_test, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted f1-score', f1_score(y_test, y_pre, labels=[1], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the precision@50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(np.argsort(np.array(y_pre)).tolist()[len(y_test) - 50:len(y_test)])\n",
    "a = list(a)\n",
    "p = []\n",
    "\n",
    "for i in range(50):\n",
    "    g = a[i]\n",
    "    p.append(y_test[g])\n",
    "    \n",
    "pre50 = np.sum(p) / 50\n",
    "\n",
    "print(pre50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Area Below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
