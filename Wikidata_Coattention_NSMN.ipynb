{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input,GRU,LSTM,Dense,Conv2D,AveragePooling1D,TimeDistributed,Flatten,MaxPooling2D,MaxPooling1D,Convolution1D,Reshape,Dropout,Embedding,Permute,Lambda,Multiply\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize glove to be the initial word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = r'glove.6B.50d.txt'\n",
    "output_file = r'gensim_glove.6B.50d.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0 = pd.read_csv(\"data_place_of_birth_0.csv\")\n",
    "f_1 = pd.read_csv('data_place_of_birth_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_0 = f_0[:500]\n",
    "f_1 = f_1[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hung was born in taipei county on 7 april 1948...</td>\n",
       "      <td>place of birth Taipei</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gabriel quak jun yi born 22 december 1990 is a...</td>\n",
       "      <td>place of birth Singapore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kapoor was born in mumbai to film producer sur...</td>\n",
       "      <td>place of birth Mumbai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dunn was born in medina, ohio and spent much o...</td>\n",
       "      <td>place of birth Medina</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the fourth of five children born to celia barb...</td>\n",
       "      <td>place of birth Royal Oak</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>gentiloni was born in rome in 1954, during his...</td>\n",
       "      <td>place of birth Rome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>gareth richard thomas born 15 july 1967 is a b...</td>\n",
       "      <td>place of birth Harrow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>wu yonggang was born in shanghai in 1907, but ...</td>\n",
       "      <td>place of birth thessaloniki</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>he was born in couvet in neuchâtel now switzer...</td>\n",
       "      <td>place of birth Couvet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>aung san suu kyi was born on 19 june 1945 in r...</td>\n",
       "      <td>place of birth cherkasy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "0    hung was born in taipei county on 7 april 1948...   \n",
       "1    gabriel quak jun yi born 22 december 1990 is a...   \n",
       "2    kapoor was born in mumbai to film producer sur...   \n",
       "3    dunn was born in medina, ohio and spent much o...   \n",
       "4    the fourth of five children born to celia barb...   \n",
       "..                                                 ...   \n",
       "995  gentiloni was born in rome in 1954, during his...   \n",
       "996  gareth richard thomas born 15 july 1967 is a b...   \n",
       "997  wu yonggang was born in shanghai in 1907, but ...   \n",
       "998  he was born in couvet in neuchâtel now switzer...   \n",
       "999  aung san suu kyi was born on 19 june 1945 in r...   \n",
       "\n",
       "                        wikidata  label  \n",
       "0          place of birth Taipei      0  \n",
       "1       place of birth Singapore      0  \n",
       "2          place of birth Mumbai      0  \n",
       "3          place of birth Medina      0  \n",
       "4       place of birth Royal Oak      0  \n",
       "..                           ...    ...  \n",
       "995          place of birth Rome      0  \n",
       "996        place of birth Harrow      0  \n",
       "997  place of birth thessaloniki      1  \n",
       "998        place of birth Couvet      0  \n",
       "999      place of birth cherkasy      1  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = f_0.append(f_1)\n",
    "f = shuffle(f)\n",
    "f = f.reset_index(drop=True)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = f[\"sentence\"].tolist()\n",
    "wikid = f[\"wikidata\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the stopwords for sentences and wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "EngStopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "drop_stop = []\n",
    "for p in range(0,len(plain)):\n",
    "    j = []\n",
    "    lower = plain[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    drop_stop.append(d)\n",
    "    \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata = []\n",
    "for p in range(0,len(wikid)):\n",
    "    j = []\n",
    "    lower = wikid[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    \n",
    "    wikidata.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "plain=[]\n",
    "\n",
    "for i in range(0,len(drop_stop)):\n",
    "    tokens = word_tokenize(drop_stop[i])  \n",
    "    tagged_sent = nltk.pos_tag(tokens)    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) \n",
    "    delimiter = ' '\n",
    "    ff = delimiter.join(lemmas_sent)\n",
    "    plain.append(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let wikidata and sentence to have their GloVe word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = len(wikidata)\n",
    "sl = len(plain)\n",
    "plain_e = []\n",
    "wikidata_e = []\n",
    "\n",
    "for i in range(len(wikidata)):\n",
    "    a = wikidata[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > wl:\n",
    "        w = w[0:wl]\n",
    "    else:\n",
    "        for k in range(wl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    wikidata_e.append(w)\n",
    "    \n",
    "for i in range(len(plain)):\n",
    "    a = plain[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > sl:\n",
    "        w = w[0:sl]\n",
    "    else:\n",
    "        for k in range(sl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    plain_e.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Layer\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nsmnattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(nsmnattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(nsmnattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        U = Permute((2,1))(x[0])\n",
    "        V = Permute((2,1))(x[1])\n",
    "        print(\"U.shape\",U.shape)\n",
    "        print(\"V.shape\",V.shape)\n",
    "        \n",
    "        E = k.batch_dot(Permute((2,1))(U),V)\n",
    "        \n",
    "        print(\"E.shape\",E.shape)\n",
    "        \n",
    "        U1 = k.batch_dot(V,Permute((2,1))((E)))     \n",
    "        \n",
    "        V1 = k.batch_dot(U,E)\n",
    "\n",
    "        U = Permute((2,1))(U)\n",
    "        U1 = Permute((2,1))(U1)\n",
    "        V = Permute((2,1))(V)\n",
    "        V1 = Permute((2,1))(V1)\n",
    "        S = Permute((2,1))((tf.keras.layers.concatenate([U,U1,(U-U1),Multiply()([U,U1])])))\n",
    "        T = Permute((2,1))((tf.keras.layers.concatenate([V,V1,(V-V1),Multiply()([V,V1])])))\n",
    "        print(\"S.shape\",S.shape)\n",
    "        print(\"T.shape\",T.shape)\n",
    "                        \n",
    "        P = LSTM(10,return_sequences=True)(S)\n",
    "        Q = LSTM(10,return_sequences=True)(T)\n",
    "        print(\"P.shape\",P.shape)\n",
    "        print(\"Q.shape\",Q.shape)\n",
    "                  \n",
    "        p = MaxPooling1D((40))(P)\n",
    "        q = MaxPooling1D((40))(Q)\n",
    "        \n",
    "        print(\"p.shape\",p.shape)\n",
    "        print(\"q.shape\",q.shape)\n",
    "        \n",
    "        m = tf.keras.layers.concatenate([p,q,(p-q),Multiply()([p,q])])\n",
    "        print(\"m.shape\",m.shape)\n",
    "        print('')\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(coattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(coattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        C = x[0]\n",
    "       \n",
    "        print(\"C.shape\",C.shape)\n",
    "        RNN=Permute((2,1))(x[1])\n",
    "        \n",
    "        f = k.dot(C,self.kernelW)\n",
    "        print(\"f.shape\",f.shape)\n",
    "        F = k.tanh(k.batch_dot(f,RNN))\n",
    "        print(\"F.shape\",F.shape)\n",
    "        \n",
    "        s = k.dot(RNN,self.kernelWs)\n",
    "        print(\"s.shape\",s.shape)\n",
    "        c = k.dot(Permute((2,1))(C),self.kernelWc)\n",
    "        print(\"c.shape\",c.shape)\n",
    "       \n",
    "        Hs = k.tanh(s+k.batch_dot(c,F))\n",
    "        print(\"Hs.shape\",Hs.shape)\n",
    "        Hc = k.tanh(c+k.batch_dot(s,Permute((2,1))(F)))\n",
    "        print(\"Hc.shape\",Hc.shape)\n",
    "        \n",
    "        \n",
    "        As = k.softmax(k.dot(Permute((2,1))(Hs),self.kernelas))\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = k.softmax(k.dot(Permute((2,1))(Hc),self.kernelac))\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        As = Permute((2,1))(As)\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = Permute((2,1))(Ac)\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        sfinal = k.batch_dot(As,Permute((2,1))(RNN))\n",
    "        print(\"sfinal.shape\",sfinal.shape)\n",
    "        \n",
    "        cfinal = k.batch_dot(Ac,C)\n",
    "        print(\"cfinal.shape\",cfinal.shape)\n",
    "        print('')\n",
    "        \n",
    "        return tf.keras.layers.concatenate([sfinal,cfinal])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(plain_e, y , test_size=0.2)\n",
    "X_train_1, X_test_1, y_train, y_test = train_test_split(wikidata_e, y , test_size=0.2)\n",
    "\n",
    "y_test_raw = y_test\n",
    "\n",
    "y_train = to_categorical(y_train,2)\n",
    "y_train = y_train.astype('int')\n",
    "y_train = y_train.reshape(-1, 1, 2)\n",
    "\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.astype('int')\n",
    "y_test = y_test.reshape(-1, 1, 2)\n",
    "\n",
    "wl = len(wikidata)\n",
    "sl = len(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "winput = Input(shape=(sl,50))\n",
    "wembed = LSTM(10,return_sequences=True)(winput)\n",
    "\n",
    "winput_1 = Input(shape=(wl,50))\n",
    "wembed_1 = LSTM(10,return_sequences=True)(winput_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.shape (None, 10, 1000)\n",
      "V.shape (None, 10, 1000)\n",
      "E.shape (None, 1000, 1000)\n",
      "S.shape (None, 40, 1000)\n",
      "T.shape (None, 40, 1000)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "P.shape (None, 40, 10)\n",
      "Q.shape (None, 40, 10)\n",
      "p.shape (None, 1, 10)\n",
      "q.shape (None, 1, 10)\n",
      "m.shape (None, 1, 40)\n",
      "\n",
      "C.shape (None, 1000, 10)\n",
      "f.shape (None, 1000, 10)\n",
      "F.shape (None, 1000, 1000)\n",
      "s.shape (None, 10, 1000)\n",
      "c.shape (None, 10, 1000)\n",
      "Hs.shape (None, 10, 1000)\n",
      "Hc.shape (None, 10, 1000)\n",
      "As.shape (None, 1000, 1)\n",
      "Ac.shape (None, 1000, 1)\n",
      "As.shape (None, 1, 1000)\n",
      "Ac.shape (None, 1, 1000)\n",
      "sfinal.shape (None, 1, 10)\n",
      "cfinal.shape (None, 1, 10)\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000, 50)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1000, 50)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 1000, 10)     2440        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1000, 10)     2440        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "nsmnattention (nsmnattention)   (None, 1, 40)        2000120     lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "coattention (coattention)       (None, 1, 20)        2000120     lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 2)         82          nsmnattention[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 2)         42          coattention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 4)         0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 2)         10          concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 2)         6           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,005,260\n",
      "Trainable params: 2,005,140\n",
      "Non-trainable params: 2,000,120\n",
      "__________________________________________________________________________________________________\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/10\n",
      "640/640 [==============================] - ETA: 0s - loss: 3.9675 - accuracy: 0.5078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 94s 147ms/sample - loss: 3.9675 - accuracy: 0.5078 - val_loss: 1.5881 - val_accuracy: 0.5375\n",
      "Epoch 2/10\n",
      "640/640 [==============================] - 94s 147ms/sample - loss: 0.9624 - accuracy: 0.4906 - val_loss: 0.7186 - val_accuracy: 0.4563\n",
      "Epoch 3/10\n",
      "640/640 [==============================] - 95s 148ms/sample - loss: 0.7045 - accuracy: 0.5172 - val_loss: 0.6953 - val_accuracy: 0.5312\n",
      "Epoch 4/10\n",
      "640/640 [==============================] - 94s 147ms/sample - loss: 0.7057 - accuracy: 0.4750 - val_loss: 0.7068 - val_accuracy: 0.4625\n",
      "Epoch 5/10\n",
      "640/640 [==============================] - 97s 151ms/sample - loss: 0.7033 - accuracy: 0.5047 - val_loss: 0.6976 - val_accuracy: 0.4750\n",
      "Epoch 6/10\n",
      "640/640 [==============================] - 93s 146ms/sample - loss: 0.7003 - accuracy: 0.4531 - val_loss: 0.6918 - val_accuracy: 0.5375\n",
      "Epoch 7/10\n",
      "640/640 [==============================] - 93s 145ms/sample - loss: 0.6986 - accuracy: 0.4891 - val_loss: 0.6927 - val_accuracy: 0.5250\n",
      "Epoch 8/10\n",
      "640/640 [==============================] - 96s 150ms/sample - loss: 0.6963 - accuracy: 0.4406 - val_loss: 0.6951 - val_accuracy: 0.4688\n",
      "Epoch 9/10\n",
      "640/640 [==============================] - 91s 143ms/sample - loss: 0.6954 - accuracy: 0.5031 - val_loss: 0.6948 - val_accuracy: 0.4812\n",
      "Epoch 10/10\n",
      "640/640 [==============================] - 94s 146ms/sample - loss: 0.6943 - accuracy: 0.4688 - val_loss: 0.6927 - val_accuracy: 0.5500\n",
      "[0.6930612993240356, 0.495]\n"
     ]
    }
   ],
   "source": [
    "co = nsmnattention(40)([wembed, wembed_1])\n",
    "co = Dense(2)(co)\n",
    "coc = coattention(20)([wembed, wembed_1])\n",
    "coc = Dense(2)(coc)\n",
    "c = tf.keras.layers.concatenate([co, coc])\n",
    "output = Dense(2)(c)\n",
    "output = Dense(2, activation=\"softmax\")(output)\n",
    "\n",
    "model = Model([winput, winput_1], [output])\n",
    "model.summary()\n",
    "\n",
    "RMSprop = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=RMSprop, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "history = model.fit([np.array(X_train), np.array(X_train_1)], [np.array(y_train)],\n",
    "                  epochs=10, validation_split=0.2, callbacks=[early_stopping], batch_size=64)\n",
    "\n",
    "scores = model.evaluate([np.array(X_test), np.array(X_test_1)], np.array(y_test), verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model.predict([np.array(X_test), np.array(X_test_1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.49797034, 0.50202966]],\n",
       "\n",
       "       [[0.48971954, 0.51028043]],\n",
       "\n",
       "       [[0.49656814, 0.50343186]],\n",
       "\n",
       "       [[0.49344563, 0.50655437]],\n",
       "\n",
       "       [[0.4944617 , 0.5055383 ]],\n",
       "\n",
       "       [[0.49997666, 0.50002337]],\n",
       "\n",
       "       [[0.49838796, 0.50161207]],\n",
       "\n",
       "       [[0.49425948, 0.5057405 ]],\n",
       "\n",
       "       [[0.4986196 , 0.5013804 ]],\n",
       "\n",
       "       [[0.49459282, 0.5054072 ]],\n",
       "\n",
       "       [[0.49311376, 0.5068863 ]],\n",
       "\n",
       "       [[0.4948397 , 0.5051603 ]],\n",
       "\n",
       "       [[0.5005489 , 0.49945107]],\n",
       "\n",
       "       [[0.49756595, 0.502434  ]],\n",
       "\n",
       "       [[0.4926059 , 0.50739413]],\n",
       "\n",
       "       [[0.5068034 , 0.49319655]],\n",
       "\n",
       "       [[0.4989382 , 0.5010618 ]],\n",
       "\n",
       "       [[0.495491  , 0.504509  ]],\n",
       "\n",
       "       [[0.49937186, 0.5006281 ]],\n",
       "\n",
       "       [[0.50165564, 0.49834436]],\n",
       "\n",
       "       [[0.4960939 , 0.5039061 ]],\n",
       "\n",
       "       [[0.49537   , 0.50463   ]],\n",
       "\n",
       "       [[0.49937174, 0.50062823]],\n",
       "\n",
       "       [[0.49305427, 0.5069457 ]],\n",
       "\n",
       "       [[0.51555246, 0.48444754]],\n",
       "\n",
       "       [[0.50334793, 0.49665213]],\n",
       "\n",
       "       [[0.49244836, 0.50755167]],\n",
       "\n",
       "       [[0.5143197 , 0.48568025]],\n",
       "\n",
       "       [[0.4971806 , 0.5028194 ]],\n",
       "\n",
       "       [[0.49727848, 0.5027215 ]],\n",
       "\n",
       "       [[0.4924153 , 0.5075847 ]],\n",
       "\n",
       "       [[0.49629816, 0.5037018 ]],\n",
       "\n",
       "       [[0.5027694 , 0.49723056]],\n",
       "\n",
       "       [[0.49468905, 0.50531095]],\n",
       "\n",
       "       [[0.49936587, 0.50063413]],\n",
       "\n",
       "       [[0.49926957, 0.5007304 ]],\n",
       "\n",
       "       [[0.50124544, 0.49875456]],\n",
       "\n",
       "       [[0.4987228 , 0.5012772 ]],\n",
       "\n",
       "       [[0.5054073 , 0.49459273]],\n",
       "\n",
       "       [[0.49532   , 0.50468004]],\n",
       "\n",
       "       [[0.49713877, 0.50286126]],\n",
       "\n",
       "       [[0.4945754 , 0.50542456]],\n",
       "\n",
       "       [[0.49762174, 0.5023783 ]],\n",
       "\n",
       "       [[0.4905167 , 0.5094833 ]],\n",
       "\n",
       "       [[0.4987046 , 0.5012954 ]],\n",
       "\n",
       "       [[0.49839312, 0.5016069 ]],\n",
       "\n",
       "       [[0.49846795, 0.5015321 ]],\n",
       "\n",
       "       [[0.49597374, 0.5040263 ]],\n",
       "\n",
       "       [[0.49984154, 0.5001584 ]],\n",
       "\n",
       "       [[0.500022  , 0.49997798]],\n",
       "\n",
       "       [[0.4960363 , 0.5039637 ]],\n",
       "\n",
       "       [[0.49568704, 0.504313  ]],\n",
       "\n",
       "       [[0.49585363, 0.50414634]],\n",
       "\n",
       "       [[0.497381  , 0.50261897]],\n",
       "\n",
       "       [[0.5047768 , 0.4952232 ]],\n",
       "\n",
       "       [[0.4949944 , 0.50500554]],\n",
       "\n",
       "       [[0.49146163, 0.50853837]],\n",
       "\n",
       "       [[0.49683708, 0.5031629 ]],\n",
       "\n",
       "       [[0.4994507 , 0.5005493 ]],\n",
       "\n",
       "       [[0.4987968 , 0.5012032 ]],\n",
       "\n",
       "       [[0.49943164, 0.5005684 ]],\n",
       "\n",
       "       [[0.49932945, 0.50067055]],\n",
       "\n",
       "       [[0.5024727 , 0.4975273 ]],\n",
       "\n",
       "       [[0.49177018, 0.5082298 ]],\n",
       "\n",
       "       [[0.49613294, 0.50386703]],\n",
       "\n",
       "       [[0.4949854 , 0.50501454]],\n",
       "\n",
       "       [[0.5014666 , 0.4985334 ]],\n",
       "\n",
       "       [[0.5007576 , 0.4992424 ]],\n",
       "\n",
       "       [[0.5047829 , 0.49521703]],\n",
       "\n",
       "       [[0.49340457, 0.50659543]],\n",
       "\n",
       "       [[0.49053708, 0.5094629 ]],\n",
       "\n",
       "       [[0.50056046, 0.4994396 ]],\n",
       "\n",
       "       [[0.5029697 , 0.49703035]],\n",
       "\n",
       "       [[0.49822113, 0.50177884]],\n",
       "\n",
       "       [[0.49299446, 0.5070056 ]],\n",
       "\n",
       "       [[0.4932775 , 0.5067225 ]],\n",
       "\n",
       "       [[0.49824473, 0.5017553 ]],\n",
       "\n",
       "       [[0.4931898 , 0.5068102 ]],\n",
       "\n",
       "       [[0.50387335, 0.4961267 ]],\n",
       "\n",
       "       [[0.49529824, 0.50470173]],\n",
       "\n",
       "       [[0.5066831 , 0.49331692]],\n",
       "\n",
       "       [[0.4947361 , 0.5052639 ]],\n",
       "\n",
       "       [[0.49547765, 0.5045224 ]],\n",
       "\n",
       "       [[0.50435644, 0.49564356]],\n",
       "\n",
       "       [[0.49743828, 0.50256175]],\n",
       "\n",
       "       [[0.49346453, 0.50653553]],\n",
       "\n",
       "       [[0.49664664, 0.50335336]],\n",
       "\n",
       "       [[0.4967534 , 0.5032466 ]],\n",
       "\n",
       "       [[0.49479964, 0.5052004 ]],\n",
       "\n",
       "       [[0.49542642, 0.5045736 ]],\n",
       "\n",
       "       [[0.49376628, 0.5062337 ]],\n",
       "\n",
       "       [[0.50469977, 0.49530026]],\n",
       "\n",
       "       [[0.4970805 , 0.50291944]],\n",
       "\n",
       "       [[0.5014196 , 0.4985804 ]],\n",
       "\n",
       "       [[0.49821624, 0.5017837 ]],\n",
       "\n",
       "       [[0.49318388, 0.5068161 ]],\n",
       "\n",
       "       [[0.5007984 , 0.4992016 ]],\n",
       "\n",
       "       [[0.49383995, 0.50616   ]],\n",
       "\n",
       "       [[0.49292272, 0.5070773 ]],\n",
       "\n",
       "       [[0.51043075, 0.48956928]],\n",
       "\n",
       "       [[0.49713686, 0.5028631 ]],\n",
       "\n",
       "       [[0.4931895 , 0.5068104 ]],\n",
       "\n",
       "       [[0.4997294 , 0.5002706 ]],\n",
       "\n",
       "       [[0.4971392 , 0.5028608 ]],\n",
       "\n",
       "       [[0.4957022 , 0.5042978 ]],\n",
       "\n",
       "       [[0.49898323, 0.5010168 ]],\n",
       "\n",
       "       [[0.4864381 , 0.5135619 ]],\n",
       "\n",
       "       [[0.49149883, 0.5085012 ]],\n",
       "\n",
       "       [[0.49649817, 0.50350183]],\n",
       "\n",
       "       [[0.5003742 , 0.4996258 ]],\n",
       "\n",
       "       [[0.49403432, 0.50596565]],\n",
       "\n",
       "       [[0.49333858, 0.5066614 ]],\n",
       "\n",
       "       [[0.4961065 , 0.5038935 ]],\n",
       "\n",
       "       [[0.49584347, 0.50415653]],\n",
       "\n",
       "       [[0.49587262, 0.5041274 ]],\n",
       "\n",
       "       [[0.5006601 , 0.49933988]],\n",
       "\n",
       "       [[0.49647412, 0.50352585]],\n",
       "\n",
       "       [[0.48813286, 0.5118671 ]],\n",
       "\n",
       "       [[0.49315223, 0.50684774]],\n",
       "\n",
       "       [[0.49332783, 0.5066722 ]],\n",
       "\n",
       "       [[0.49176058, 0.50823945]],\n",
       "\n",
       "       [[0.49669915, 0.50330085]],\n",
       "\n",
       "       [[0.49507618, 0.5049238 ]],\n",
       "\n",
       "       [[0.5030494 , 0.4969506 ]],\n",
       "\n",
       "       [[0.50018233, 0.49981764]],\n",
       "\n",
       "       [[0.4949834 , 0.50501657]],\n",
       "\n",
       "       [[0.4919375 , 0.50806254]],\n",
       "\n",
       "       [[0.49579394, 0.50420606]],\n",
       "\n",
       "       [[0.49855348, 0.5014465 ]],\n",
       "\n",
       "       [[0.48921937, 0.51078063]],\n",
       "\n",
       "       [[0.49668208, 0.50331795]],\n",
       "\n",
       "       [[0.49713716, 0.5028628 ]],\n",
       "\n",
       "       [[0.48585054, 0.5141494 ]],\n",
       "\n",
       "       [[0.4945251 , 0.5054749 ]],\n",
       "\n",
       "       [[0.49902108, 0.50097895]],\n",
       "\n",
       "       [[0.49471506, 0.5052849 ]],\n",
       "\n",
       "       [[0.49694282, 0.50305724]],\n",
       "\n",
       "       [[0.4962222 , 0.5037778 ]],\n",
       "\n",
       "       [[0.4978772 , 0.5021228 ]],\n",
       "\n",
       "       [[0.4959337 , 0.5040663 ]],\n",
       "\n",
       "       [[0.4981084 , 0.5018916 ]],\n",
       "\n",
       "       [[0.4919156 , 0.50808436]],\n",
       "\n",
       "       [[0.49660078, 0.50339925]],\n",
       "\n",
       "       [[0.509515  , 0.49048504]],\n",
       "\n",
       "       [[0.49999976, 0.50000024]],\n",
       "\n",
       "       [[0.49918884, 0.50081116]],\n",
       "\n",
       "       [[0.49798012, 0.5020199 ]],\n",
       "\n",
       "       [[0.49953198, 0.500468  ]],\n",
       "\n",
       "       [[0.49541536, 0.5045846 ]],\n",
       "\n",
       "       [[0.4988149 , 0.5011851 ]],\n",
       "\n",
       "       [[0.49475276, 0.50524724]],\n",
       "\n",
       "       [[0.49749708, 0.5025029 ]],\n",
       "\n",
       "       [[0.49572033, 0.5042796 ]],\n",
       "\n",
       "       [[0.4968958 , 0.5031042 ]],\n",
       "\n",
       "       [[0.4951263 , 0.5048737 ]],\n",
       "\n",
       "       [[0.49718428, 0.5028157 ]],\n",
       "\n",
       "       [[0.49763277, 0.50236726]],\n",
       "\n",
       "       [[0.49792477, 0.5020752 ]],\n",
       "\n",
       "       [[0.4955259 , 0.50447416]],\n",
       "\n",
       "       [[0.4952207 , 0.50477934]],\n",
       "\n",
       "       [[0.49740258, 0.50259745]],\n",
       "\n",
       "       [[0.49931034, 0.5006897 ]],\n",
       "\n",
       "       [[0.50614417, 0.49385577]],\n",
       "\n",
       "       [[0.49321386, 0.50678617]],\n",
       "\n",
       "       [[0.49866474, 0.5013353 ]],\n",
       "\n",
       "       [[0.49577022, 0.5042298 ]],\n",
       "\n",
       "       [[0.49991903, 0.500081  ]],\n",
       "\n",
       "       [[0.50203145, 0.49796852]],\n",
       "\n",
       "       [[0.49582952, 0.5041705 ]],\n",
       "\n",
       "       [[0.4991712 , 0.5008288 ]],\n",
       "\n",
       "       [[0.5001822 , 0.49981776]],\n",
       "\n",
       "       [[0.5078442 , 0.49215576]],\n",
       "\n",
       "       [[0.49643442, 0.5035656 ]],\n",
       "\n",
       "       [[0.50101197, 0.49898803]],\n",
       "\n",
       "       [[0.50077325, 0.49922678]],\n",
       "\n",
       "       [[0.49479064, 0.5052093 ]],\n",
       "\n",
       "       [[0.49533793, 0.5046621 ]],\n",
       "\n",
       "       [[0.4935966 , 0.5064034 ]],\n",
       "\n",
       "       [[0.49759224, 0.5024078 ]],\n",
       "\n",
       "       [[0.50142956, 0.49857044]],\n",
       "\n",
       "       [[0.4983255 , 0.5016745 ]],\n",
       "\n",
       "       [[0.4957235 , 0.50427645]],\n",
       "\n",
       "       [[0.4988318 , 0.5011682 ]],\n",
       "\n",
       "       [[0.49742478, 0.50257516]],\n",
       "\n",
       "       [[0.493861  , 0.50613904]],\n",
       "\n",
       "       [[0.4955983 , 0.5044017 ]],\n",
       "\n",
       "       [[0.49991688, 0.50008315]],\n",
       "\n",
       "       [[0.49867973, 0.50132024]],\n",
       "\n",
       "       [[0.5004278 , 0.4995722 ]],\n",
       "\n",
       "       [[0.4994776 , 0.5005224 ]],\n",
       "\n",
       "       [[0.5018936 , 0.49810642]],\n",
       "\n",
       "       [[0.4998561 , 0.5001439 ]],\n",
       "\n",
       "       [[0.49537262, 0.50462735]],\n",
       "\n",
       "       [[0.49548554, 0.5045145 ]],\n",
       "\n",
       "       [[0.49654475, 0.5034553 ]],\n",
       "\n",
       "       [[0.48657104, 0.5134289 ]],\n",
       "\n",
       "       [[0.49975896, 0.50024104]],\n",
       "\n",
       "       [[0.5023591 , 0.49764094]],\n",
       "\n",
       "       [[0.49785674, 0.5021432 ]],\n",
       "\n",
       "       [[0.4972756 , 0.50272447]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22 84]\n",
      " [17 77]]\n",
      "Weighted precision 0.4782608695652174\n",
      "Weighted recall 0.8191489361702128\n",
      "Weighted f1-score 0.603921568627451\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_pre = []\n",
    "\n",
    "for i in range(len(pre)):\n",
    "    w = np.argmax(pre[i])\n",
    "    y_pre.append(w)\n",
    "\n",
    "print(confusion_matrix(y_test_raw, y_pre))\n",
    "\n",
    "print('Weighted precision', precision_score(y_test_raw, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted recall', recall_score(y_test_raw, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted f1-score', f1_score(y_test_raw, y_pre, labels=[1], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the precision@50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = set(np.argsort(np.array(y_pre)).tolist()[len(y_test) - 50:len(y_test)])\n",
    "a = list(a)\n",
    "p = []\n",
    "\n",
    "for i in range(50):\n",
    "    g = a[i]\n",
    "    p.append(y_test[g])\n",
    "    \n",
    "pre50 = np.sum(p) / 50\n",
    "\n",
    "print(pre50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Area Below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
