{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input,GRU,LSTM,Dense,Conv2D,AveragePooling1D,TimeDistributed,Flatten,MaxPooling2D,MaxPooling1D,Convolution1D,Reshape,Dropout,Embedding,Permute,Lambda,Multiply\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize glove to be the initial word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = r'glove.6B.50d.txt'\n",
    "output_file = r'gensim_glove.6B.50d.txt'\n",
    "glove2word2vec(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv(\"data_place_of_birth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lin was born in houguan , which is around pres...</td>\n",
       "      <td>place of birth debrzno</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>born in the village of ibogun-olaogun to a far...</td>\n",
       "      <td>place of birth prague</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>paul hintze was born in 1864 in the little tow...</td>\n",
       "      <td>place of birth prague</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>chen yang is a chinese tv and radio personalit...</td>\n",
       "      <td>place of birth daegu</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>montgomery was born at irvine in ayrshire in s...</td>\n",
       "      <td>place of birth allendale</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>gauthier grumier born 29 may 1984, in nevers i...</td>\n",
       "      <td>place of birth san severino marche</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>stephen heller was born in pest now budapest, ...</td>\n",
       "      <td>place of birth suzhou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>fukuda was born on february 4, 1932 in tokyo t...</td>\n",
       "      <td>place of birth hexham</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>matteo salvini was born in milan in 1973, the ...</td>\n",
       "      <td>place of birth funabashi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>blankers-koen was born on 26 april 1918 in lag...</td>\n",
       "      <td>place of birth des moines</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "6    lin was born in houguan , which is around pres...   \n",
       "16   born in the village of ibogun-olaogun to a far...   \n",
       "18   paul hintze was born in 1864 in the little tow...   \n",
       "28   chen yang is a chinese tv and radio personalit...   \n",
       "30   montgomery was born at irvine in ayrshire in s...   \n",
       "..                                                 ...   \n",
       "975  gauthier grumier born 29 may 1984, in nevers i...   \n",
       "976  stephen heller was born in pest now budapest, ...   \n",
       "983  fukuda was born on february 4, 1932 in tokyo t...   \n",
       "988  matteo salvini was born in milan in 1973, the ...   \n",
       "994  blankers-koen was born on 26 april 1918 in lag...   \n",
       "\n",
       "                               wikidata  label  \n",
       "6                place of birth debrzno      1  \n",
       "16                place of birth prague      1  \n",
       "18                place of birth prague      1  \n",
       "28                 place of birth daegu      1  \n",
       "30             place of birth allendale      1  \n",
       "..                                  ...    ...  \n",
       "975  place of birth san severino marche      1  \n",
       "976               place of birth suzhou      1  \n",
       "983               place of birth hexham      1  \n",
       "988            place of birth funabashi      1  \n",
       "994           place of birth des moines      1  \n",
       "\n",
       "[344 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[f['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain = f[\"sentence\"].tolist()\n",
    "wikid = f[\"wikidata\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the stopwords for sentences and wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "EngStopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "drop_stop = []\n",
    "for p in range(0,len(plain)):\n",
    "    j = []\n",
    "    lower = plain[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    drop_stop.append(d)\n",
    "    \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata = []\n",
    "for p in range(0,len(wikid)):\n",
    "    j = []\n",
    "    lower = wikid[p].lower()\n",
    "    for word in lower.split():\n",
    "        if word in EngStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            j.append(word)\n",
    "    \n",
    "    d = j[0]\n",
    "    for i in range(1,len(j)):\n",
    "        d = d + \" \" + j[i]\n",
    "    \n",
    "    wikidata.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "plain=[]\n",
    "\n",
    "for i in range(0,len(drop_stop)):\n",
    "    tokens = word_tokenize(drop_stop[i])  \n",
    "    tagged_sent = nltk.pos_tag(tokens)    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) \n",
    "    delimiter = ' '\n",
    "    ff = delimiter.join(lemmas_sent)\n",
    "    plain.append(ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let wikidata and sentence to have their GloVe word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = len(wikidata)\n",
    "sl = len(plain)\n",
    "plain_e = []\n",
    "wikidata_e = []\n",
    "\n",
    "for i in range(len(wikidata)):\n",
    "    a = wikidata[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > wl:\n",
    "        w = w[0:wl]\n",
    "    else:\n",
    "        for k in range(wl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    wikidata_e.append(w)\n",
    "    \n",
    "for i in range(len(plain)):\n",
    "    a = plain[i].split()\n",
    "    w = []\n",
    "    for j in range(len(a)):\n",
    "        try:\n",
    "            w.append(model[a[j]].tolist())\n",
    "        except:\n",
    "            w.append([0]*50)\n",
    "    if len(w) > sl:\n",
    "        w = w[0:sl]\n",
    "    else:\n",
    "        for k in range(sl-len(w)):\n",
    "            w.append([0]*50)\n",
    "    plain_e.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Layer\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nsmnattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(nsmnattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False)\n",
    "        super(nsmnattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        U = Permute((2,1))(x[0])\n",
    "        V = Permute((2,1))(x[1])\n",
    "        print(\"U.shape\",U.shape)\n",
    "        print(\"V.shape\",V.shape)\n",
    "        \n",
    "        E = k.batch_dot(Permute((2,1))(U),V)\n",
    "        \n",
    "        print(\"E.shape\",E.shape)\n",
    "        \n",
    "        U1 = k.batch_dot(V,Permute((2,1))((E)))     \n",
    "        \n",
    "        V1 = k.batch_dot(U,E)\n",
    "\n",
    "        U = Permute((2,1))(U)\n",
    "        U1 = Permute((2,1))(U1)\n",
    "        V = Permute((2,1))(V)\n",
    "        V1 = Permute((2,1))(V1)\n",
    "        S = Permute((2,1))((tf.keras.layers.concatenate([U,U1,(U-U1),Multiply()([U,U1])])))\n",
    "        T = Permute((2,1))((tf.keras.layers.concatenate([V,V1,(V-V1),Multiply()([V,V1])])))\n",
    "        print(\"S.shape\",S.shape)\n",
    "        print(\"T.shape\",T.shape)\n",
    "                        \n",
    "        P = LSTM(10,return_sequences=True)(S)\n",
    "        Q = LSTM(10,return_sequences=True)(T)\n",
    "        print(\"P.shape\",P.shape)\n",
    "        print(\"Q.shape\",Q.shape)\n",
    "                  \n",
    "        p = MaxPooling1D((40))(P)\n",
    "        q = MaxPooling1D((40))(Q)\n",
    "        \n",
    "        print(\"p.shape\",p.shape)\n",
    "        print(\"q.shape\",q.shape)\n",
    "        \n",
    "        m = tf.keras.layers.concatenate([p,q,(p-q),Multiply()([p,q])])\n",
    "        print(\"m.shape\",m.shape)\n",
    "        print('')\n",
    "        \n",
    "        return m\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coattention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(coattention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        \n",
    "        self.kernelW = self.add_weight(name='Wall', \n",
    "                                      shape=(10, 10),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWs = self.add_weight(name='Ws', \n",
    "                                      shape=(wl,wl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelWc = self.add_weight(name='Wc', \n",
    "                                      shape=(sl,sl),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelas = self.add_weight(name='Was', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.kernelac = self.add_weight(name='Wac', \n",
    "                                      shape=(10,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(coattention, self).build(input_shape)  \n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        C = x[0]\n",
    "       \n",
    "        print(\"C.shape\",C.shape)\n",
    "        RNN=Permute((2,1))(x[1])\n",
    "        \n",
    "        f = k.dot(C,self.kernelW)\n",
    "        print(\"f.shape\",f.shape)\n",
    "        F = k.tanh(k.batch_dot(f,RNN))\n",
    "        print(\"F.shape\",F.shape)\n",
    "        \n",
    "        s = k.dot(RNN,self.kernelWs)\n",
    "        print(\"s.shape\",s.shape)\n",
    "        c = k.dot(Permute((2,1))(C),self.kernelWc)\n",
    "        print(\"c.shape\",c.shape)\n",
    "       \n",
    "        Hs = k.tanh(s+k.batch_dot(c,F))\n",
    "        print(\"Hs.shape\",Hs.shape)\n",
    "        Hc = k.tanh(c+k.batch_dot(s,Permute((2,1))(F)))\n",
    "        print(\"Hc.shape\",Hc.shape)\n",
    "        \n",
    "        \n",
    "        As = k.softmax(k.dot(Permute((2,1))(Hs),self.kernelas))\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = k.softmax(k.dot(Permute((2,1))(Hc),self.kernelac))\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        As = Permute((2,1))(As)\n",
    "        print(\"As.shape\",As.shape)\n",
    "        Ac = Permute((2,1))(Ac)\n",
    "        print(\"Ac.shape\",Ac.shape)\n",
    "        \n",
    "        sfinal = k.batch_dot(As,Permute((2,1))(RNN))\n",
    "        print(\"sfinal.shape\",sfinal.shape)\n",
    "        \n",
    "        cfinal = k.batch_dot(Ac,C)\n",
    "        print(\"cfinal.shape\",cfinal.shape)\n",
    "        print('')\n",
    "        \n",
    "        return tf.keras.layers.concatenate([sfinal,cfinal])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(plain_e, y , test_size=0.2, random_state=1000)\n",
    "X_train_1, X_test_1, y_train, y_test = train_test_split(wikidata_e, y , test_size=0.2, random_state=1000)\n",
    "\n",
    "y_train = to_categorical(y_train,2)\n",
    "y_train = y_train.astype('int')\n",
    "y_train = y_train.reshape(-1, 1, 2)\n",
    "y_test = to_categorical(y_test,2)\n",
    "y_test = y_test.astype('int')\n",
    "y_test = y_test.reshape(-1, 1, 2)\n",
    "\n",
    "wl = len(wikidata)\n",
    "sl = len(plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "winput = Input(shape=(sl,50))\n",
    "wembed = LSTM(10,return_sequences=True)(winput)\n",
    "\n",
    "winput_1 = Input(shape=(wl,50))\n",
    "wembed_1 = LSTM(10,return_sequences=True)(winput_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.shape (None, 10, 1000)\n",
      "V.shape (None, 10, 1000)\n",
      "E.shape (None, 1000, 1000)\n",
      "S.shape (None, 40, 1000)\n",
      "T.shape (None, 40, 1000)\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "P.shape (None, 40, 10)\n",
      "Q.shape (None, 40, 10)\n",
      "p.shape (None, 1, 10)\n",
      "q.shape (None, 1, 10)\n",
      "m.shape (None, 1, 40)\n",
      "\n",
      "C.shape (None, 1000, 10)\n",
      "f.shape (None, 1000, 10)\n",
      "F.shape (None, 1000, 1000)\n",
      "s.shape (None, 10, 1000)\n",
      "c.shape (None, 10, 1000)\n",
      "Hs.shape (None, 10, 1000)\n",
      "Hc.shape (None, 10, 1000)\n",
      "As.shape (None, 1000, 1)\n",
      "Ac.shape (None, 1000, 1)\n",
      "As.shape (None, 1, 1000)\n",
      "Ac.shape (None, 1, 1000)\n",
      "sfinal.shape (None, 1, 10)\n",
      "cfinal.shape (None, 1, 10)\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000, 50)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1000, 50)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 1000, 10)     2440        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1000, 10)     2440        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "nsmnattention (nsmnattention)   (None, 1, 40)        2000120     lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "coattention (coattention)       (None, 1, 20)        2000120     lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 2)         82          nsmnattention[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 2)         42          coattention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 4)         0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 2)         10          concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 2)         6           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,005,260\n",
      "Trainable params: 2,005,140\n",
      "Non-trainable params: 2,000,120\n",
      "__________________________________________________________________________________________________\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/20\n",
      "640/640 [==============================] - ETA: 0s - loss: 5.9729 - accuracy: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640/640 [==============================] - 93s 145ms/sample - loss: 5.9729 - accuracy: 0.0781 - val_loss: 2.2141 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "640/640 [==============================] - 93s 145ms/sample - loss: 2.1556 - accuracy: 0.0000e+00 - val_loss: 0.9973 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "640/640 [==============================] - 92s 144ms/sample - loss: 1.0855 - accuracy: 0.0000e+00 - val_loss: 0.8594 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "640/640 [==============================] - 93s 145ms/sample - loss: 0.7290 - accuracy: 0.0000e+00 - val_loss: 0.7079 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "640/640 [==============================] - 96s 150ms/sample - loss: 0.6641 - accuracy: 0.0000e+00 - val_loss: 0.6425 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "640/640 [==============================] - 92s 144ms/sample - loss: 0.6452 - accuracy: 0.0000e+00 - val_loss: 0.6525 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "640/640 [==============================] - 94s 147ms/sample - loss: 0.6455 - accuracy: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "640/640 [==============================] - 96s 150ms/sample - loss: 0.6356 - accuracy: 0.0000e+00 - val_loss: 0.6344 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "640/640 [==============================] - 98s 152ms/sample - loss: 0.6350 - accuracy: 0.0000e+00 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "640/640 [==============================] - 93s 146ms/sample - loss: 0.6311 - accuracy: 0.0000e+00 - val_loss: 0.6363 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "640/640 [==============================] - 97s 152ms/sample - loss: 0.6290 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "640/640 [==============================] - 92s 144ms/sample - loss: 0.6217 - accuracy: 0.0000e+00 - val_loss: 0.6322 - val_accuracy: 0.0000e+00\n",
      "Epoch 00012: early stopping\n",
      "[0.6449865627288819, 0.0]\n"
     ]
    }
   ],
   "source": [
    "co = nsmnattention(40)([wembed, wembed_1])\n",
    "co = Dense(2)(co)\n",
    "coc = coattention(20)([wembed, wembed_1])\n",
    "coc = Dense(2)(coc)\n",
    "c = tf.keras.layers.concatenate([co, coc])\n",
    "output = Dense(2)(c)\n",
    "output = Dense(2, activation=\"softmax\")(output)\n",
    "\n",
    "model = Model([winput, winput_1], [output])\n",
    "model.summary()\n",
    "\n",
    "RMSprop = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=RMSprop, loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.Accuracy()], experimental_run_tf_function=False)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2)\n",
    "\n",
    "history = model.fit([np.array(X_train), np.array(X_train_1)], [np.array(y_train)],\n",
    "                  epochs=20, validation_split=0.2, callbacks=[early_stopping], batch_size=64)\n",
    "\n",
    "scores = model.evaluate([np.array(X_test), np.array(X_test_1)], np.array(y_test), verbose=0)\n",
    "pre = model.predict([np.array(X_test), np.array(X_test_1)])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[134   0]\n",
      " [ 66   0]]\n",
      "Weighted precision 0.0\n",
      "Weighted recall 0.0\n",
      "Weighted f1-score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_pre = []\n",
    "\n",
    "for i in range(len(pre)):\n",
    "    k = pre[i]\n",
    "    w = np.where(k == np.max(k))[0][0].tolist()\n",
    "    y_pre.append(w)\n",
    "\n",
    "    \n",
    "y = f[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(plain_e, y, test_size=0.2, random_state=1000)\n",
    "X_train_1, X_test_1, y_train, y_test = train_test_split(wikidata_e, y, test_size=0.2, random_state=1000)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pre))\n",
    "\n",
    "print('Weighted precision', precision_score(y_test, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted recall', recall_score(y_test, y_pre, labels=[1], average='macro'))\n",
    "print('Weighted f1-score', f1_score(y_test, y_pre, labels=[1], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the precision@50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28\n"
     ]
    }
   ],
   "source": [
    "a = set(np.argsort(np.array(y_pre)).tolist()[len(y_test) - 50:len(y_test)])\n",
    "a = list(a)\n",
    "p = []\n",
    "\n",
    "for i in range(50):\n",
    "    g = a[i]\n",
    "    p.append(y_test[g])\n",
    "    \n",
    "pre50 = np.sum(p) / 50\n",
    "\n",
    "print(pre50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Area Below"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
