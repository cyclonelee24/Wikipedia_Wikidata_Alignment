{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version :  1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"  \n",
    "\n",
    "# import the tokenizer which is used on this pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch version : \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  28996\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"vocab size : \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'yokozeki was born in osaka prefecture on september 11, 1979'\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# print(text)\n",
    "# print(tokens[:10], '...')\n",
    "# print(ids[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_place_of_birth / data_place_of_death / data_occupation\n",
    "\n",
    "data_0 = pd.read_csv('data_place_of_birth_0.csv')\n",
    "data_1 = pd.read_csv('data_place_of_birth_1.csv')\n",
    "data_2 = pd.read_csv('data_POB_POD_OCC_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0_train = data_0[0:900]\n",
    "data_0_val = data_0[900:1800]\n",
    "data_0_test = data_0[1800:2700]\n",
    "\n",
    "data_1_train = data_1[0:100]\n",
    "data_1_val = data_1[100:200]\n",
    "data_1_test = data_1[200:300]\n",
    "\n",
    "data_2_train = data_2[0:1000]\n",
    "data_2_val = data_2[1000:2000]\n",
    "data_2_test = data_2[2000:3000]\n",
    "\n",
    "frame_train = [data_0_train, data_1_train, data_2_train]\n",
    "frame_val = [data_0_val, data_1_val, data_2_val]\n",
    "frame_test = [data_0_test, data_1_test, data_2_test]\n",
    "\n",
    "df_train = shuffle(pd.concat(frame_train, ignore_index=True))\n",
    "df_val = shuffle(pd.concat(frame_val, ignore_index=True))\n",
    "df_test = shuffle(pd.concat(frame_test, ignore_index=True))\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (2000, 3)\n",
      "Validation Set: (2000, 3)\n",
      "Test Set: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\"% df_train.columns, df_train.shape)\n",
    "print(\"Validation Set:\"% df_val.columns, df_val.shape)\n",
    "print(\"Test Set:\"% df_test.columns, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>van niekerk was born in kraaifontein, cape tow...</td>\n",
       "      <td>place of birth Cape Town</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shui junyi , born september 20, 1963 in sanjia...</td>\n",
       "      <td>place of birth Lanzhou</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huang's family was from meixian, guangdong but...</td>\n",
       "      <td>place of birth Tianjin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>born in hitchin, hertfordshire, kitson spent h...</td>\n",
       "      <td>place of birth Hitchin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in june 1841 he completed his first scientific...</td>\n",
       "      <td>place of birth Heilbronn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  van niekerk was born in kraaifontein, cape tow...   \n",
       "1  shui junyi , born september 20, 1963 in sanjia...   \n",
       "2  huang's family was from meixian, guangdong but...   \n",
       "3  born in hitchin, hertfordshire, kitson spent h...   \n",
       "4  in june 1841 he completed his first scientific...   \n",
       "\n",
       "                   wikidata  label  \n",
       "0  place of birth Cape Town      0  \n",
       "1    place of birth Lanzhou      0  \n",
       "2    place of birth Tianjin      0  \n",
       "3    place of birth Hitchin      0  \n",
       "4  place of birth Heilbronn      2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data\n",
      "Number of sentences :  2000\n",
      "Longest sentence's length : 1247\n",
      "Average length of the sentences : 131.105\n"
     ]
    }
   ],
   "source": [
    "print('For train data')\n",
    "print(\"Number of sentences : \", len(df_train))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_train.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_train.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For validation data\n",
      "Number of sentences :  2000\n",
      "Longest sentence's length : 1456\n",
      "Average length of the sentences : 133.755\n"
     ]
    }
   ],
   "source": [
    "print('For validation data')\n",
    "print(\"Number of sentences : \", len(df_val))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_val.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_val.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data\n",
      "Number of sentences :  2000\n",
      "Longest sentence's length : 725\n",
      "Average length of the sentences : 130.024\n"
     ]
    }
   ],
   "source": [
    "print('For test data')\n",
    "print(\"Number of sentences : \", len(df_test))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_test.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_test.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the tweet max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_text_length(text, length):\n",
    "    text = text[:length]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he was one of the sons of saint clotilda, born...</td>\n",
       "      <td>place of birth Reims</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kim han-sol was born in pyongyang in 1995 and ...</td>\n",
       "      <td>place of birth Pyongyang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>john robin warren ac born 11 june 1937, in ade...</td>\n",
       "      <td>place of birth Adelaide</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the couple had four children; art was the olde...</td>\n",
       "      <td>place of birth Toledo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"neil harbisson, ciborg de colors\", catalunya ...</td>\n",
       "      <td>place of birth Mataró</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>allam was born in egypt and raised by muslim p...</td>\n",
       "      <td>place of birth london</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>his former teams are nec nijmegen, roda jc, fc...</td>\n",
       "      <td>place of birth Assen</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>born august 6, 1990 in tokyo is a japanese act...</td>\n",
       "      <td>place of birth Tokyo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>wiles states that he came across fermat's last...</td>\n",
       "      <td>place of birth Cambridge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>telfer was born in carluke and grew up support...</td>\n",
       "      <td>place of birth Carluke</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     he was one of the sons of saint clotilda, born...   \n",
       "1     kim han-sol was born in pyongyang in 1995 and ...   \n",
       "2     john robin warren ac born 11 june 1937, in ade...   \n",
       "3     the couple had four children; art was the olde...   \n",
       "4     \"neil harbisson, ciborg de colors\", catalunya ...   \n",
       "...                                                 ...   \n",
       "1995  allam was born in egypt and raised by muslim p...   \n",
       "1996  his former teams are nec nijmegen, roda jc, fc...   \n",
       "1997  born august 6, 1990 in tokyo is a japanese act...   \n",
       "1998  wiles states that he came across fermat's last...   \n",
       "1999  telfer was born in carluke and grew up support...   \n",
       "\n",
       "                      wikidata  label  \n",
       "0         place of birth Reims      0  \n",
       "1     place of birth Pyongyang      0  \n",
       "2      place of birth Adelaide      0  \n",
       "3        place of birth Toledo      0  \n",
       "4        place of birth Mataró      2  \n",
       "...                        ...    ...  \n",
       "1995     place of birth london      1  \n",
       "1996      place of birth Assen      2  \n",
       "1997      place of birth Tokyo      0  \n",
       "1998  place of birth Cambridge      2  \n",
       "1999    place of birth Carluke      0  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentence'] = df_test['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pinkerton was not raised in a religious upbrin...</td>\n",
       "      <td>place of birth Glasgow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his father was a māori farmer and artist of te...</td>\n",
       "      <td>place of birth Raukokore</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maas was born in saarlouis to a catholic famil...</td>\n",
       "      <td>place of birth Saarlouis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he became the first brazilian badminton player...</td>\n",
       "      <td>place of birth Campinas</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in 2000 he resigned from his position on micro...</td>\n",
       "      <td>place of birth Seattle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>berg graduated from henderson high school in w...</td>\n",
       "      <td>place of birth Philadelphia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>baumgarten was born in berlin as the fifth of ...</td>\n",
       "      <td>place of birth Berlin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>sayers, her life and soul london hodder and st...</td>\n",
       "      <td>place of birth Oxford</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>muro was born in yanaka, musashi province mode...</td>\n",
       "      <td>place of birth Yanaka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>\"the only holocaust survivor to serve in the u...</td>\n",
       "      <td>place of birth Budapest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     pinkerton was not raised in a religious upbrin...   \n",
       "1     his father was a māori farmer and artist of te...   \n",
       "2     maas was born in saarlouis to a catholic famil...   \n",
       "3     he became the first brazilian badminton player...   \n",
       "4     in 2000 he resigned from his position on micro...   \n",
       "...                                                 ...   \n",
       "1995  berg graduated from henderson high school in w...   \n",
       "1996  baumgarten was born in berlin as the fifth of ...   \n",
       "1997  sayers, her life and soul london hodder and st...   \n",
       "1998  muro was born in yanaka, musashi province mode...   \n",
       "1999  \"the only holocaust survivor to serve in the u...   \n",
       "\n",
       "                         wikidata  label  \n",
       "0          place of birth Glasgow      2  \n",
       "1        place of birth Raukokore      2  \n",
       "2        place of birth Saarlouis      0  \n",
       "3         place of birth Campinas      2  \n",
       "4          place of birth Seattle      2  \n",
       "...                           ...    ...  \n",
       "1995  place of birth Philadelphia      2  \n",
       "1996        place of birth Berlin      0  \n",
       "1997        place of birth Oxford      2  \n",
       "1998        place of birth Yanaka      0  \n",
       "1999      place of birth Budapest      0  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['sentence'] = df_val['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences that exceed 300 characters\n",
      "68\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>van niekerk was born in kraaifontein, cape tow...</td>\n",
       "      <td>place of birth Cape Town</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shui junyi , born september 20, 1963 in sanjia...</td>\n",
       "      <td>place of birth Lanzhou</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huang's family was from meixian, guangdong but...</td>\n",
       "      <td>place of birth Tianjin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>born in hitchin, hertfordshire, kitson spent h...</td>\n",
       "      <td>place of birth Hitchin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in june 1841 he completed his first scientific...</td>\n",
       "      <td>place of birth Heilbronn</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>until he was pressured to retire shortly befor...</td>\n",
       "      <td>place of birth Greenwich</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>lindenberg started his musical career as a dru...</td>\n",
       "      <td>place of birth Gronau</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>mott was born in leeds to lilian mary reynolds...</td>\n",
       "      <td>place of birth Leeds</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>rafael eduardo medina born february 15, 1975 i...</td>\n",
       "      <td>place of birth Panama City</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>his son osama bin laden was the notorious foun...</td>\n",
       "      <td>place of birth Hadhramaut</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1932 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     van niekerk was born in kraaifontein, cape tow...   \n",
       "1     shui junyi , born september 20, 1963 in sanjia...   \n",
       "2     huang's family was from meixian, guangdong but...   \n",
       "3     born in hitchin, hertfordshire, kitson spent h...   \n",
       "4     in june 1841 he completed his first scientific...   \n",
       "...                                                 ...   \n",
       "1995  until he was pressured to retire shortly befor...   \n",
       "1996  lindenberg started his musical career as a dru...   \n",
       "1997  mott was born in leeds to lilian mary reynolds...   \n",
       "1998  rafael eduardo medina born february 15, 1975 i...   \n",
       "1999  his son osama bin laden was the notorious foun...   \n",
       "\n",
       "                        wikidata  label  \n",
       "0       place of birth Cape Town      0  \n",
       "1         place of birth Lanzhou      0  \n",
       "2         place of birth Tianjin      0  \n",
       "3         place of birth Hitchin      0  \n",
       "4       place of birth Heilbronn      2  \n",
       "...                          ...    ...  \n",
       "1995    place of birth Greenwich      2  \n",
       "1996       place of birth Gronau      2  \n",
       "1997        place of birth Leeds      0  \n",
       "1998  place of birth Panama City      0  \n",
       "1999   place of birth Hadhramaut      2  \n",
       "\n",
       "[1932 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 2000\n",
    "MAX_LENGTH = 300\n",
    "print('Number of sentences that exceed 300 characters')\n",
    "print(L - len(df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]))\n",
    "\n",
    "# print(\"Number of tweets : \", len(df_train))\n",
    "# df_train['sentence'] = df_train['sentence'].map(lambda x: max_text_length(x, 300))\n",
    "df_train = df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_val.to_csv(\"validation.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van niekerk was born in kraaifontein, cape town, to wayne van niekerk and sprinter odessa swarts.wayde's olympic glory what his parents have to say, iol he was born prematurely and needed a blood transfusion\n",
      "place of birth Cape Town\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "T = pd.read_csv('train.csv')\n",
    "T1, T2, T3= T.iloc[0, :].values\n",
    "print(T1)\n",
    "print(T2)\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    # read the tsv we make and initialize some parameters\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"validation\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \".csv\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # use BERT tokenizer\n",
    "    \n",
    "    # define a function that reutrn a training or testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\" or self.mode == \"validation\":\n",
    "            sentence, wikidata = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            sentence, wikidata, label_id = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(sentence)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        tokens_b = self.tokenizer.tokenize(wikidata)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # convert hole token sequence into index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # segments_tensor\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "trainset = WikiDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original document]\n",
      "sentence  ：van niekerk was born in kraaifontein, cape town, to wayne van niekerk and sprinter odessa swarts.wayde's olympic glory what his parents have to say, iol he was born prematurely and needed a blood transfusion\n",
      "wikidata  ：place of birth Cape Town\n",
      "label ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[ tensors return by Dataset ]\n",
      "tokens_tensor  ：tensor([  101,  3498, 11437,  4820,  1200,  1377,  1108,  1255,  1107,   180,\n",
      "         1611,  3814, 14467, 11656,  1394,   117, 23546,  1411,   117,  1106,\n",
      "         1236,  1673,  3498, 11437,  4820,  1200,  1377,  1105, 24360,   184,\n",
      "         4704,  3202,   188, 18320,  1116,   119,  1236,  2007,   112,   188,\n",
      "          184,  1193,  8223,  1596, 12887,  1184,  1117,  2153,  1138,  1106,\n",
      "         1474,   117,   178,  4063,  1119,  1108,  1255, 24505,  1193,  1105,\n",
      "         1834,   170,  1892, 14715, 17149,   102,  1282,  1104,  3485,  4343,\n",
      "         2779,   102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：0\n",
      "\n",
      "--------------------\n",
      "\n",
      "[tokens_tensors word pieces]\n",
      "[CLS]vanni##ek##er##kwasbornink##ra##ai##fo##nte##in,capetown,toway##nevanni##ek##er##kandsprintero##des##sas##wart##s.way##de'so##ly##mp##icglorywhathisparentshavetosay,i##olhewasbornpremature##lyandneededabloodtrans##fusion[SEP]placeofbirthCapeTown[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select first sample\n",
    "sample_idx = 0\n",
    "\n",
    "# compare with the original document\n",
    "sentence_0, wikidata_0, label_0 = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# use the Dataset we built to extract the transformed id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# convert the tokens_tensor to original document\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[original document]\n",
    "sentence  ：{sentence_0}\n",
    "wikidata  ：{wikidata_0}\n",
    "label ：{label_0}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[ tensors return by Dataset ]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[tokens_tensors word pieces]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a DataLoader that can return a mini-batch each time\n",
    "This DataLoader works with the 'WikiDataset' we define previously\n",
    "we need 4 tensors when training a BERT model：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The input samples of this function is a list,\n",
    "# every element in it is a sample return by the 'WikiDataset'\n",
    "\n",
    "# Every sample contains 3 tensors : \n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "\n",
    "# It will procecss zero padding on the first two tensors,\n",
    "# then create a masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # with labels or not\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pading\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # attention masks, \n",
    "    # set the locations that are not zero padding tokens_tensors to 1 in order to let bert only focus on those tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataLoader\n",
    "# use `collate_fn` to combine list of samples into a mini-batch\n",
    "BATCH_SIZE = 24\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([24, 86]) \n",
      "tensor([[  101,  3498, 11437,  ...,     0,     0,     0],\n",
      "        [  101,   188, 23618,  ...,     0,     0,     0],\n",
      "        [  101,   177, 25530,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   175, 25081,  ...,     0,     0,     0],\n",
      "        [  101, 22904,  1320,  ...,     0,     0,     0],\n",
      "        [  101,  3840,  9238,  ...,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([24, 86])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([24, 86])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([24])\n",
      "tensor([0, 0, 0, 0, 2, 2, 2, 1, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 3,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # first 3 tensors are tokens, segments and masks \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # calculate accuracy when training\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # record current batch\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# run the model on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "# _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "# print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of parameters of whole model：108312579\n",
      "number of parameters of the linear classifier：2307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "number of parameters of whole model：{sum(p.numel() for p in model_params)}\n",
    "number of parameters of the linear classifier：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 47.299, acc: 0.918\n",
      "[epoch 2] loss: 17.139, acc: 0.954\n",
      "[epoch 3] loss: 11.167, acc: 0.974\n",
      "[epoch 4] loss: 8.062, acc: 0.980\n",
      "[epoch 5] loss: 6.243, acc: 0.986\n",
      "[epoch 6] loss: 4.715, acc: 0.986\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "\n",
    "# using Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 6    # number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # record batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # calculate accuracy\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'B:/Documents/2020五上/Wikidata/Wikipedia_Wikidata_Alignment/model_place_of_birth'\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiset = WikiDataset(\"validation\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiloader = DataLoader(valiset, batch_size=32, collate_fn=create_mini_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    validations = get_predictions(model, valiloader)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations_numpy = validations.cpu().clone().numpy()\n",
    "validations_numpy = validations_numpy.reshape((2000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1,2001)\n",
    "ids = ids.reshape((2000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    2],\n",
       "       [   2,    2],\n",
       "       [   3,    0],\n",
       "       ...,\n",
       "       [1998,    2],\n",
       "       [1999,    0],\n",
       "       [2000,    0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations_array = np.concatenate((ids, validations_numpy), axis=1)\n",
    "validations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validations = pd.DataFrame(data = validations_array, columns=[\"id\", \"label\"])\n",
    "df_validations['label'] = df_validations['label'].map({0:0, 1:1, 2:2})\n",
    "# df_validations.to_csv('answer.txt', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       0\n",
       "3       2\n",
       "4       2\n",
       "       ..\n",
       "1995    2\n",
       "1996    0\n",
       "1997    2\n",
       "1998    0\n",
       "1999    0\n",
       "Name: label, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validations['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 :  0.9714806325693123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 : \",f1_score(df_validations['label'], df_val['label'],  average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[893   0   7]\n",
      " [  0 100   0]\n",
      " [ 50   0 950]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(df_val['label'], df_validations['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       900\n",
      "           1       1.00      1.00      1.00       100\n",
      "           2       0.99      0.95      0.97      1000\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_val['label'], df_validations['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
