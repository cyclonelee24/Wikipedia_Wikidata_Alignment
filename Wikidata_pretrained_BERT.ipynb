{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version :  1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"  \n",
    "\n",
    "# import the tokenizer which is used on this pretrained model\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch version : \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  28996\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"vocab size : \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'yokozeki was born in osaka prefecture on september 11, 1979'\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# print(text)\n",
    "# print(tokens[:10], '...')\n",
    "# print(ids[:10], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_place_of_birth / data_place_of_death / data_occupation\n",
    "\n",
    "data_0 = pd.read_csv('./data/data_place_of_birth_0.csv')\n",
    "data_1 = pd.read_csv('./data/data_place_of_birth_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0_train = data_0[0:1500]\n",
    "data_0_val = data_0[1500:3000]\n",
    "data_0_test = data_0[3000:4500]\n",
    "\n",
    "data_1_train = data_1[0:1500]\n",
    "data_1_val = data_1[1500:3000]\n",
    "data_1_test = data_1[3000:4500]\n",
    "\n",
    "df_train = shuffle(data_0_train.append(data_1_train, ignore_index=True))\n",
    "df_val = shuffle(data_0_val.append(data_1_val, ignore_index=True))\n",
    "df_test = shuffle(data_0_test.append(data_1_test, ignore_index=True))\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (3000, 3)\n",
      "Validation Set: (3000, 3)\n",
      "Test Set: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\"% df_train.columns, df_train.shape)\n",
    "print(\"Validation Set:\"% df_val.columns, df_val.shape)\n",
    "print(\"Test Set:\"% df_test.columns, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>juan antonio ramos sánchez born 18 august 1976...</td>\n",
       "      <td>place of birth biên hòa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>born into the prominent borgia family in xàtiv...</td>\n",
       "      <td>place of birth Xàtiva</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virat kohli was born on 5 november 1988 in del...</td>\n",
       "      <td>place of birth Delhi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>born in manchester, wellens started his career...</td>\n",
       "      <td>place of birth kikuka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>du ping , born 28 february 1978, in shenyang i...</td>\n",
       "      <td>place of birth yokohama</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence                 wikidata  \\\n",
       "0  juan antonio ramos sánchez born 18 august 1976...  place of birth biên hòa   \n",
       "1  born into the prominent borgia family in xàtiv...    place of birth Xàtiva   \n",
       "2  virat kohli was born on 5 november 1988 in del...     place of birth Delhi   \n",
       "3  born in manchester, wellens started his career...    place of birth kikuka   \n",
       "4  du ping , born 28 february 1978, in shenyang i...  place of birth yokohama   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data\n",
      "Number of sentences :  3000\n",
      "Longest sentence's length : 1456\n",
      "Average length of the sentences : 132.28466666666668\n"
     ]
    }
   ],
   "source": [
    "print('For train data')\n",
    "print(\"Number of sentences : \", len(df_train))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_train.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_train.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For validation data\n",
      "Number of sentences :  3000\n",
      "Longest sentence's length : 2473\n",
      "Average length of the sentences : 132.42366666666666\n"
     ]
    }
   ],
   "source": [
    "print('For validation data')\n",
    "print(\"Number of sentences : \", len(df_val))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_val.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_val.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data\n",
      "Number of sentences :  3000\n",
      "Longest sentence's length : 1456\n",
      "Average length of the sentences : 132.74033333333333\n"
     ]
    }
   ],
   "source": [
    "print('For test data')\n",
    "print(\"Number of sentences : \", len(df_test))\n",
    "print(\"Longest sentence\\'s length : \" + str(df_test.sentence.map(len).max()))\n",
    "print(\"Average length of the sentences : \" + str(df_test.sentence.map(len).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the tweet max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_text_length(text, length):\n",
    "    text = text[:length]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>born in pencoed, dicomidis progressed through ...</td>\n",
       "      <td>place of birth Pencoed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scott william taylor was born in baltimore and...</td>\n",
       "      <td>place of birth miami</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but at the time of the birth of yao beina, he ...</td>\n",
       "      <td>place of birth Wuhan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quinn was born in london in 1964 to a french m...</td>\n",
       "      <td>place of birth kanagawa prefecture</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>melek hu, born hou meiling hou mei ling transf...</td>\n",
       "      <td>place of birth Shenyang</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>brendan o'hara born 27 april 1963birth certifi...</td>\n",
       "      <td>place of birth Glasgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>charles was born at the château de vendôme, el...</td>\n",
       "      <td>place of birth Vendôme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>kapoor was born on 28 september 1982 in bombay...</td>\n",
       "      <td>place of birth kōtō-ku</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>giovanni battista pamphili was born in rome on...</td>\n",
       "      <td>place of birth Rome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>gordon muir campbell, born january 12, 1948 is...</td>\n",
       "      <td>place of birth daejeon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     born in pencoed, dicomidis progressed through ...   \n",
       "1     scott william taylor was born in baltimore and...   \n",
       "2     but at the time of the birth of yao beina, he ...   \n",
       "3     quinn was born in london in 1964 to a french m...   \n",
       "4     melek hu, born hou meiling hou mei ling transf...   \n",
       "...                                                 ...   \n",
       "2995  brendan o'hara born 27 april 1963birth certifi...   \n",
       "2996  charles was born at the château de vendôme, el...   \n",
       "2997  kapoor was born on 28 september 1982 in bombay...   \n",
       "2998  giovanni battista pamphili was born in rome on...   \n",
       "2999  gordon muir campbell, born january 12, 1948 is...   \n",
       "\n",
       "                                wikidata  label  \n",
       "0                 place of birth Pencoed      0  \n",
       "1                   place of birth miami      1  \n",
       "2                   place of birth Wuhan      0  \n",
       "3     place of birth kanagawa prefecture      1  \n",
       "4                place of birth Shenyang      0  \n",
       "...                                  ...    ...  \n",
       "2995              place of birth Glasgow      0  \n",
       "2996              place of birth Vendôme      0  \n",
       "2997              place of birth kōtō-ku      1  \n",
       "2998                 place of birth Rome      0  \n",
       "2999              place of birth daejeon      1  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['sentence'] = df_test['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chou tien-chen chinese 周天成; born 8 january 199...</td>\n",
       "      <td>place of birth derbent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speke was born on 4 may 1827 at orleigh court,...</td>\n",
       "      <td>place of birth Bideford</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>born in caen, calvados, costil started his car...</td>\n",
       "      <td>place of birth Caen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lansbury was born in enfield, greater london a...</td>\n",
       "      <td>place of birth London</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rupert was born at amberg in the upper palatin...</td>\n",
       "      <td>place of birth seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>courbet was born in abbeville as the youngest ...</td>\n",
       "      <td>place of birth thetford</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>the elder son rostislav born in 1992 is the st...</td>\n",
       "      <td>place of birth Kiev</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>born in naples to a family of italian and dist...</td>\n",
       "      <td>place of birth guangzhou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>one of five children, earnshaw was born on the...</td>\n",
       "      <td>place of birth Mufulira</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>born in rome to parents from missanello, basil...</td>\n",
       "      <td>place of birth Rome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     chou tien-chen chinese 周天成; born 8 january 199...   \n",
       "1     speke was born on 4 may 1827 at orleigh court,...   \n",
       "2     born in caen, calvados, costil started his car...   \n",
       "3     lansbury was born in enfield, greater london a...   \n",
       "4     rupert was born at amberg in the upper palatin...   \n",
       "...                                                 ...   \n",
       "2995  courbet was born in abbeville as the youngest ...   \n",
       "2996  the elder son rostislav born in 1992 is the st...   \n",
       "2997  born in naples to a family of italian and dist...   \n",
       "2998  one of five children, earnshaw was born on the...   \n",
       "2999  born in rome to parents from missanello, basil...   \n",
       "\n",
       "                      wikidata  label  \n",
       "0       place of birth derbent      1  \n",
       "1      place of birth Bideford      0  \n",
       "2          place of birth Caen      0  \n",
       "3        place of birth London      0  \n",
       "4       place of birth seattle      1  \n",
       "...                        ...    ...  \n",
       "2995   place of birth thetford      1  \n",
       "2996       place of birth Kiev      0  \n",
       "2997  place of birth guangzhou      1  \n",
       "2998   place of birth Mufulira      0  \n",
       "2999       place of birth Rome      0  \n",
       "\n",
       "[3000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['sentence'] = df_val['sentence'].map(lambda x: max_text_length(x, 1200))\n",
    "\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences that exceed 300 characters\n",
      "3103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>wikidata</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>juan antonio ramos sánchez born 18 august 1976...</td>\n",
       "      <td>place of birth biên hòa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>born into the prominent borgia family in xàtiv...</td>\n",
       "      <td>place of birth Xàtiva</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virat kohli was born on 5 november 1988 in del...</td>\n",
       "      <td>place of birth Delhi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>born in manchester, wellens started his career...</td>\n",
       "      <td>place of birth kikuka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>du ping , born 28 february 1978, in shenyang i...</td>\n",
       "      <td>place of birth yokohama</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>tyutchev was born into a russian noble family ...</td>\n",
       "      <td>place of birth budapest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>the second child of gustav josef kokoschka, a ...</td>\n",
       "      <td>place of birth seoul</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>james edward day was born in jacksonville, ill...</td>\n",
       "      <td>place of birth Jacksonville</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>federico franco was born in the city of asunci...</td>\n",
       "      <td>place of birth Asunción</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>min jin , born 24 may 1978 in wuhan is a chine...</td>\n",
       "      <td>place of birth Wuhan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2897 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     juan antonio ramos sánchez born 18 august 1976...   \n",
       "1     born into the prominent borgia family in xàtiv...   \n",
       "2     virat kohli was born on 5 november 1988 in del...   \n",
       "3     born in manchester, wellens started his career...   \n",
       "4     du ping , born 28 february 1978, in shenyang i...   \n",
       "...                                                 ...   \n",
       "2994  tyutchev was born into a russian noble family ...   \n",
       "2995  the second child of gustav josef kokoschka, a ...   \n",
       "2996  james edward day was born in jacksonville, ill...   \n",
       "2997  federico franco was born in the city of asunci...   \n",
       "2999  min jin , born 24 may 1978 in wuhan is a chine...   \n",
       "\n",
       "                         wikidata  label  \n",
       "0         place of birth biên hòa      1  \n",
       "1           place of birth Xàtiva      0  \n",
       "2            place of birth Delhi      0  \n",
       "3           place of birth kikuka      1  \n",
       "4         place of birth yokohama      1  \n",
       "...                           ...    ...  \n",
       "2994      place of birth budapest      1  \n",
       "2995         place of birth seoul      1  \n",
       "2996  place of birth Jacksonville      0  \n",
       "2997      place of birth Asunción      0  \n",
       "2999         place of birth Wuhan      0  \n",
       "\n",
       "[2897 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 6000\n",
    "MAX_LENGTH = 300\n",
    "print('Number of sentences that exceed 300 characters')\n",
    "print(L - len(df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]))\n",
    "\n",
    "# print(\"Number of tweets : \", len(df_train))\n",
    "# df_train['sentence'] = df_train['sentence'].map(lambda x: max_text_length(x, 300))\n",
    "df_train = df_train[~(df_train.sentence.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train.csv\", index=False)\n",
    "df_val.to_csv(\"validation.csv\", index=False)\n",
    "df_test.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan antonio ramos sánchez born 18 august 1976 in barcelona is a spanish taekwondo practitioner\n",
      "place of birth biên hòa\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "T = pd.read_csv('train.csv')\n",
    "T1, T2, T3= T.iloc[0, :].values\n",
    "print(T1)\n",
    "print(T2)\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    # read the tsv we make and initialize some parameters\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"validation\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \".csv\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # use BERT tokenizer\n",
    "    \n",
    "    # define a function that reutrn a training or testing data\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\" or self.mode == \"validation\":\n",
    "            sentence, wikidata = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            sentence, wikidata, label_id = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # BERT tokens\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(sentence)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        tokens_b = self.tokenizer.tokenize(wikidata)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # convert hole token sequence into index sequence\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # segments_tensor\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "trainset = WikiDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[original document]\n",
      "sentence  ：juan antonio ramos sánchez born 18 august 1976 in barcelona is a spanish taekwondo practitioner\n",
      "wikidata  ：place of birth biên hòa\n",
      "label ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[ tensors return by Dataset ]\n",
      "tokens_tensor  ：tensor([  101,   179,  8734, 22904, 11153,  1186, 26084,  2155,   188,  4881,\n",
      "         4386,  1584,  1255,  1407, 12686, 12909,  1204,  2402,  1107,  2927,\n",
      "        18389,  7637,  1110,   170,  8492,  2944, 27629,  4820, 16732,  2572,\n",
      "        22351,   102,  1282,  1104,  3485, 16516, 24559,  1179,   177, 20142,\n",
      "         1161,   102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[tokens_tensors word pieces]\n",
      "[CLS]j##uanant##oni##oram##oss##án##che##zborn18au##gus##t1976inbar##cel##onaisaspan##ishta##ek##won##dopractitioner[SEP]placeofbirthbi##ê##nh##ò##a[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select first sample\n",
    "sample_idx = 0\n",
    "\n",
    "# compare with the original document\n",
    "sentence_0, wikidata_0, label_0 = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# use the Dataset we built to extract the transformed id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# convert the tokens_tensor to original document\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[original document]\n",
    "sentence  ：{sentence_0}\n",
    "wikidata  ：{wikidata_0}\n",
    "label ：{label_0}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[ tensors return by Dataset ]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[tokens_tensors word pieces]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a DataLoader that can return a mini-batch each time\n",
    "This DataLoader works with the 'WikiDataset' we define previously\n",
    "we need 4 tensors when training a BERT model：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The input samples of this function is a list,\n",
    "# every element in it is a sample return by the 'WikiDataset'\n",
    "\n",
    "# Every sample contains 3 tensors : \n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "\n",
    "# It will procecss zero padding on the first two tensors,\n",
    "# then create a masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # with labels or not\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pading\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # attention masks, \n",
    "    # set the locations that are not zero padding tokens_tensors to 1 in order to let bert only focus on those tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataLoader\n",
    "# use `collate_fn` to combine list of samples into a mini-batch\n",
    "BATCH_SIZE = 24\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([24, 77]) \n",
      "tensor([[  101,   179,  8734,  ...,     0,     0,     0],\n",
      "        [  101,  1255,  1154,  ...,     0,     0,     0],\n",
      "        [  101,   191,  5132,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1177, 27008,  ...,     0,     0,     0],\n",
      "        [  101,  1113,   179,  ...,     0,     0,     0],\n",
      "        [  101,   185,  1233,  ...,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([24, 77])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([24, 77])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([24])\n",
      "tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# n_class = 2\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # first 3 tensors are tokens, segments and masks \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # calculate accuracy when training\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # record current batch\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# run the model on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "# _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "# print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of parameters of whole model：108311810\n",
      "number of parameters of the linear classifier：1538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "number of parameters of whole model：{sum(p.numel() for p in model_params)}\n",
    "number of parameters of the linear classifier：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 24.757, acc: 0.993\n",
      "[epoch 2] loss: 1.743, acc: 0.999\n",
      "[epoch 3] loss: 1.411, acc: 1.000\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "\n",
    "# using Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 3    # number of epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # record batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # calculate accuracy\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'B:/Documents/2020五上/Wikidata/Wikipedia_Wikidata_Alignment/model_place_of_birth'\n",
    "# torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiset = WikiDataset(\"validation\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valiloader = DataLoader(valiset, batch_size=32, collate_fn=create_mini_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    validations = get_predictions(model, valiloader)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validations_numpy = validations.cpu().clone().numpy()\n",
    "validations_numpy = validations_numpy.reshape((3000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(1,3001)\n",
    "ids = ids.reshape((3000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1],\n",
       "       [   2,    0],\n",
       "       [   3,    0],\n",
       "       ...,\n",
       "       [2998,    1],\n",
       "       [2999,    0],\n",
       "       [3000,    0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validations_array = np.concatenate((ids, validations_numpy), axis=1)\n",
    "validations_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validations = pd.DataFrame(data = validations_array, columns=[\"id\", \"label\"])\n",
    "df_validations['label'] = df_validations['label'].map({0:0, 1:1})\n",
    "# df_validations.to_csv('answer.txt', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 :  0.9976666793704395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 : \",f1_score(df_validations['label'], df_val['label'],  average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1500    0]\n",
      " [   7 1493]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(df_val['label'], df_validations['label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
